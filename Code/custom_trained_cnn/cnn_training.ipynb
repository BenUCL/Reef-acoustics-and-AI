{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenUCL/Reef-acoustics-and-AI/blob/main/Code/CNN_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLRcmHXBrd50"
      },
      "source": [
        "# **Train the CNN**\n",
        "\n",
        "This script provides an example of training the CNN on the minibatch files which can be created with the 'audio_preprocessing' script. This uses a small subset of the Indonesian dataset.\n",
        "\n",
        "This outputs a csv file of predictions for each 0.96sec chunk from each audio files. These can be converted into predictions for whole minutes and a final test accuracy reported using the 'calc_ccn_acc.ipynb' script. This will be saved in the 'Results/Colab_CNN_predictions' folder.\n",
        "\n",
        "Note csv's from the full datasets are provided instead for analysis beyond this script as intensive analysis with audio files is no longer needed.\n",
        "\n",
        "# **Using Colabs free GPU feature**\n",
        "\n",
        "Google colab provides free GPU access (with some limits), see here: https://research.google.com/colaboratory/faq.html\n",
        "\n",
        "This can be used to significantly increase training speed. To switch this on go to 'Runtime' at the top and change type to 'GPU'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFSJ2V4TmVSq",
        "outputId": "fdd76008-9012-427a-b362-9e51219f1e1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connect your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h2jq6cOCT7Ca",
        "outputId": "c892978f-23dd-4a02-d490-724cb89b4c8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.21.5\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting resampy==0.2.2\n",
            "  Downloading resampy-0.2.2.tar.gz (323 kB)\n",
            "\u001b[K     |████████████████████████████████| 323 kB 54.8 MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.15\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 23 kB/s \n",
            "\u001b[?25hCollecting tf_slim==1.1.0\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 70.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
            "Collecting soundfile==0.10.3.post1\n",
            "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: scipy>=0.13 in /usr/local/lib/python3.7/dist-packages (from resampy==0.2.2) (1.7.3)\n",
            "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.7/dist-packages (from resampy==0.2.2) (0.56.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.37.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (2.0.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 63.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.49.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.14.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile==0.10.3.post1) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile==0.10.3.post1) (2.21)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.32->resampy==0.2.2) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.32->resampy==0.2.2) (5.0.0)\n",
            "Requirement already satisfied: setuptools<60 in /usr/local/lib/python3.7/dist-packages (from numba>=0.32->resampy==0.2.2) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.32->resampy==0.2.2) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.32->resampy==0.2.2) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
            "Building wheels for collected packages: resampy, gast\n",
            "  Building wheel for resampy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320732 sha256=8254c0497e475dc47ba2215213fd2461b173ea3284fc2bf4d9e41588f4bb5232\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/18/0a/8ad18a597d8333a142c9789338a96a6208f1198d290ece356c\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=17cfc02412d5d4a1094457e4d96f9974b0e9a76f50d0a8a61455dc27f7a294a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built resampy gast\n",
            "Installing collected packages: numpy, tensorflow-estimator, tensorboard, keras-applications, gast, tf-slim, tensorflow, soundfile, resampy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "  Attempting uninstall: soundfile\n",
            "    Found existing installation: soundfile 0.11.0\n",
            "    Uninstalling soundfile-0.11.0:\n",
            "      Successfully uninstalled soundfile-0.11.0\n",
            "  Attempting uninstall: resampy\n",
            "    Found existing installation: resampy 0.4.2\n",
            "    Uninstalling resampy-0.4.2:\n",
            "      Successfully uninstalled resampy-0.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 numpy-1.21.5 resampy-0.2.2 soundfile-0.10.3.post1 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 tf-slim-1.1.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" As package versions began updating this threw errors on the smoke test. \\nFor a faster download versions could be removed but this may throw errors. \\nAs of 17/10/22 it gives the below output, but, the smoketest codeblock passes:\\n\\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\\ntensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\\nkapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\\nSuccessfully installed gast-0.2.2 keras-applications-1.0.8 llvmlite-0.32.1 numba-0.49.1 numpy-1.21.5 resampy-0.2.2 soundfile-0.10.3.post1 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 tf-slim-1.1.0\\nWARNING: The following packages were previously imported in this runtime:\\n  [numpy]\\nYou must restart the runtime in order to use newly installed versions. \""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install numpy==1.21.5 resampy==0.2.2 tensorflow==1.15 tf_slim==1.1.0 six==1.15.0 soundfile==0.10.3.post1\n",
        "\n",
        "\"\"\" As package versions began updating this threw errors on the smoke test. \n",
        "For a faster download versions could be removed but this may throw errors. \n",
        "As of 17/10/22 it gives the below output, but, the smoketest codeblock passes:\n",
        "\n",
        "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
        "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
        "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\n",
        "Successfully installed gast-0.2.2 keras-applications-1.0.8 llvmlite-0.32.1 numba-0.49.1 numpy-1.21.5 resampy-0.2.2 soundfile-0.10.3.post1 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 tf-slim-1.1.0\n",
        "WARNING: The following packages were previously imported in this runtime:\n",
        "  [numpy]\n",
        "You must restart the runtime in order to use newly installed versions. \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV91rjp5T7KW",
        "outputId": "ee6a6caf-0fdc-4cec-856f-a1dfbebcc240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Reef soundscapes with AI/Audioset\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "\n",
            "Testing your install of VGGish\n",
            "\n",
            "Log Mel Spectrogram example:  [[-4.47297436 -4.29457354 -4.14940631 ... -3.9747003  -3.94774997\n",
            "  -3.78687669]\n",
            " [-4.48589533 -4.28825497 -4.139964   ... -3.98368686 -3.94976505\n",
            "  -3.7951698 ]\n",
            " [-4.46158065 -4.29329706 -4.14905953 ... -3.96442484 -3.94895483\n",
            "  -3.78619839]\n",
            " ...\n",
            " [-4.46152626 -4.29365061 -4.14848608 ... -3.96638113 -3.95057575\n",
            "  -3.78538167]\n",
            " [-4.46152595 -4.2936572  -4.14848104 ... -3.96640507 -3.95059567\n",
            "  -3.78537143]\n",
            " [-4.46152565 -4.29366386 -4.14847603 ... -3.96642906 -3.95061564\n",
            "  -3.78536116]]\n",
            "2022-10-18 14:03:33.231897: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2022-10-18 14:03:33.327189: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2022-10-18 14:03:33.327313: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (2a2e782ee2f1): /proc/driver/nvidia/version does not exist\n",
            "2022-10-18 14:03:33.327839: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-10-18 14:03:33.338832: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2022-10-18 14:03:33.339111: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4f3d500 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-10-18 14:03:33.339352: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1666: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "VGGish embedding:  [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.16137294 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.80695784\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.36792755 0.03582418 0.         0.         0.\n",
            " 0.         0.38027036 0.13755938 0.9174708  0.80656356 0.\n",
            " 0.         0.         0.         0.04036269 0.7076244  0.\n",
            " 0.4978391  0.24081807 0.21565425 0.884923   1.1956801  0.67061985\n",
            " 0.2077946  0.01639876 0.17471859 0.         0.         0.25100812\n",
            " 0.         0.         0.14607906 0.         0.39887053 0.30542108\n",
            " 0.1289675  0.         0.         0.         0.         0.\n",
            " 0.5385135  0.         0.         0.04941082 0.42527413 0.18537286\n",
            " 0.         0.         0.1475353  0.         0.         0.6993387\n",
            " 0.45541185 0.05174828 0.         0.01992539 0.         0.\n",
            " 0.5181578  0.56557596 0.6587975  0.         0.         0.41056335\n",
            " 0.         0.         0.         0.25765198 0.23232105 0.24026453\n",
            " 0.         0.         0.         0.         0.         0.2652375\n",
            " 0.         0.48460817 0.         0.         0.19325784 0.\n",
            " 0.20123352 0.         0.03368617 0.         0.         0.\n",
            " 0.         0.17836353 0.024749   0.06889973 0.         0.\n",
            " 0.         0.08246295 0.         0.         0.         0.\n",
            " 0.         0.        ]\n",
            "\n",
            "Looks Good To Me!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Should output 'Looks good to me at the bottom!'\n",
        "%cd /content/drive/MyDrive/Reef soundscapes with AI/Audioset\n",
        "!python vggish_smoke_test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZdEHlqrT7M6",
        "outputId": "66b01b62-5e52-450c-af08-584e4c520c74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "#From original vggish_train_demo.py script on github\n",
        "from __future__ import print_function\n",
        "\n",
        "from random import shuffle\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tf_slim as slim\n",
        "\n",
        "import vggish_input\n",
        "import vggish_params\n",
        "import vggish_slim\n",
        "\n",
        "#Modules added by Ben\n",
        "import os #for handling directories\n",
        "import glob #for dealing with files in dir\n",
        "import pandas as pd #for saving output at end in dataframe\n",
        "import sklearn\n",
        "import math\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split #added for train/test split\n",
        "from numpy import loadtxt #addded so predictions can be output to CSV file\n",
        "from datetime import datetime #added to append time to csv output file name to prevent overwriting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTz1EZxtiRa9"
      },
      "source": [
        "**Set paths to access modules and pickle files, also set CNN parameters.**\n",
        "\n",
        "Two classes are used here, increase _NUM_CLASSES if needed. A batch size of 16 was used as larger batches can cause a memory error on colab depending on which GPU you are  allocated. The network trains for 5 epochs currently to save computation time, the final study used UCL's computing cluster to train for 50 epochs on the full datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0zN9KAOcWziG"
      },
      "outputs": [],
      "source": [
        "#which repeat of the cross-val is this? (1-8):\n",
        "repeat = 1 # Used to set seed for train/val/test split\n",
        "\n",
        "### Change paths if you re-structure folders\n",
        "\n",
        "# Path to the location where your audio file are stored:\n",
        "audio_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/audio_dir' \n",
        "\n",
        "# Path to folder containing vggish setup files and 'AudiosetAnalysis' downloaded from sarebs supplementary\n",
        "vggish_files = r'/content/drive/MyDrive/Reef soundscapes with AI/Audioset' \n",
        "\n",
        "# Output folder for results:\n",
        "results_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/Results/Colab_CNN_predictions/' \n",
        "\n",
        "#Set the directories where logmel-spectrograms will be stored for train, test and validation sets:\n",
        "pickle_trainfiles_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/Results/minibatches_train/'\n",
        "pickle_valfiles_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/Results/minibatches_val/'\n",
        "pickle_testfiles_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/Results/minibatches_test/'\n",
        "\n",
        "#how many classes?:\n",
        "_NUM_CLASSES = 2\n",
        "\n",
        "#name a column for each class e.g 'class1', 'class2', or 'healthy', 'degraded'\n",
        "col_names = 'Healthy','Degraded', 'True class'\n",
        "\n",
        "#Batch size:\n",
        "batch_size = 16 # larger batches can cause a memory error on the NN script on colab depending on which GPU you are  allocated \n",
        "\n",
        "# Number of epochs.\n",
        "num_epochs = 5\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apTYZYvDnbZN",
        "outputId": "6fb4b11d-6c0d-459d-d2f7-98243a39f00f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of train minibatches found: 8\n",
            "Number of validation minibatches found: 1\n",
            "Number of test minibatches found: 1\n",
            "Cross validation combination: 1\n"
          ]
        }
      ],
      "source": [
        "#### Some final set up\n",
        "# Find number of minibatches for networks for loop\n",
        "minibatches = [filename for filename in os.listdir(pickle_trainfiles_dir) if filename.startswith(\"train_minibatch\")]\n",
        "num_minibatches = len(minibatches) #this takes the last digit of the last pickle files, denoting how many minibatches there are\n",
        "\n",
        "# Get number of train/test/val minibatches\n",
        "num_train_batches = int(len(os.listdir(pickle_trainfiles_dir)))\n",
        "print('Number of train minibatches found: ' + str(num_train_batches))\n",
        "num_val_batches = int(len(os.listdir(pickle_valfiles_dir)))\n",
        "print('Number of validation minibatches found: ' + str(num_val_batches))\n",
        "num_test_batches = int(len(os.listdir(pickle_testfiles_dir)))\n",
        "print('Number of test minibatches found: ' + str(num_test_batches))\n",
        "\n",
        "os.chdir(vggish_files) \n",
        "\n",
        "# Used to find averages of accuracy score across minibatches later\n",
        "def Average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "\n",
        "print('Cross validation combination: ' + str(repeat))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thPSDwZmk2Dv"
      },
      "source": [
        "# **Run the neural network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZuMo8UDkI55"
      },
      "outputs": [],
      "source": [
        "# Set arguments to pass to trianing loop\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--train_vggish\", type=bool, default=True, help=\"Allow fine-tuning VGGish\")\n",
        "parser.add_argument(\"--checkpoint\", type=str, default=\"vggish_model.ckpt\", help=\"Path to checkpoint\")\n",
        "\n",
        "args, _ = parser.parse_known_args()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dckDf8YewAX8"
      },
      "source": [
        "'An exception has occurred, use %tb to see the full traceback.' error will occur, fear not, this just means its finished "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E0YTlu4xNvt",
        "outputId": "7b8a4817-4c39-40f6-b096-a724c3d0d04a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W1018 14:03:43.141526 140056522688384 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W1018 14:03:43.251882 140056522688384 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1666: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "W1018 14:03:43.349927 140056522688384 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "I1018 14:03:48.103091 140056522688384 saver.py:1284] Restoring parameters from vggish_model.ckpt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mini batch0\n",
            "mini batch1\n",
            "mini batch2\n",
            "mini batch3\n",
            "mini batch4\n",
            "mini batch5\n",
            "mini batch6\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "\"\"\"To train 5 epochs on the 123x1min files in the of training data this process\n",
        "takes up to 80 minutes on a CPU. Depending which GPU Colab provides you this can \n",
        "take < 5min on colabs GPU.\"\"\"\n",
        "\n",
        "\"\"\"This uses the VGGish model definition within a larger model which adds two \n",
        "dense layers and then trains the full network. \n",
        "\n",
        "We input log-mel spectrograms (X_train) calculated above with associated labels \n",
        "(y_train), and feed the batches into the model. Once the model is trained, it \n",
        "is then executed on the validation and log-mel spectrograms (X_validation, \n",
        "X_test), and the accuracy is output for each.\n",
        "\n",
        "Alongside .csv file with the predictions for each 0.96s chunk and their true\n",
        "class is also output for the test data. Column1 = the logit for the first class,\n",
        "Column2 = the logit for the scond class etc. The final column is the true class.\n",
        "\n",
        "Final accuracy is actually taken from these predictions in another script which \n",
        "takes the  most common predicted class across an entire minute using each 0.96s \n",
        "chunks prediction.\"\"\"\n",
        "\n",
        "\n",
        "def main(X):   \n",
        "  with tf.Graph().as_default(), tf.Session() as sess:\n",
        "    # Define VGGish.\n",
        "    embeddings = vggish_slim.define_vggish_slim(training=args.train_vggish)\n",
        "    \n",
        "    # Define a shallow classification model and associated training ops on top\n",
        "    # of VGGish.\n",
        "    with tf.variable_scope('mymodel'):\n",
        "      # Add a fully connected layer with 100 units. Add an activation function\n",
        "      # to the embeddings since they are pre-activation.\n",
        "      num_units = 100\n",
        "      fc = slim.fully_connected(tf.nn.relu(embeddings), num_units)\n",
        "    \n",
        "      linear_out= slim.fully_connected(                                      \n",
        "          fc, _NUM_CLASSES, activation_fn=None, scope='linear_out')\n",
        "      logits = tf.sigmoid(linear_out, name='logits')\n",
        "    \n",
        "      # Add training ops.\n",
        "      with tf.variable_scope('train'):\n",
        "        global_step = tf.train.create_global_step()\n",
        "\n",
        "        # Labels are assumed to be fed as a batch multi-hot vectors, with\n",
        "        # a 1 in the position of each positive class label, and 0 elsewhere.\n",
        "        labels_input = tf.placeholder(\n",
        "            tf.float32, shape=(None, _NUM_CLASSES), name='labels')\n",
        "\n",
        "        # Cross-entropy label loss.\n",
        "        xent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=logits, labels=labels_input, name='xent')    \n",
        "        loss = tf.reduce_mean(xent, name='loss_op')\n",
        "        tf.summary.scalar('loss', loss)\n",
        "\n",
        "        # We use the same optimizer and hyperparameters as used to train VGGish.\n",
        "        optimizer = tf.train.AdamOptimizer(\n",
        "            learning_rate=vggish_params.LEARNING_RATE,\n",
        "            epsilon=vggish_params.ADAM_EPSILON)\n",
        "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
        "\n",
        "    # Initialize all variables in the model, and then load the pre-trained\n",
        "    # VGGish checkpoint.\n",
        "    sess.run(tf.global_variables_initializer())         # this starts the tf session\n",
        "    vggish_slim.load_vggish_slim_checkpoint(sess, args.checkpoint)\n",
        "\n",
        "    \n",
        "    features_input = sess.graph.get_tensor_by_name(\n",
        "        vggish_params.INPUT_TENSOR_NAME)\n",
        "    \n",
        "    # The training loop.\n",
        "    highest_acc_score = 0\n",
        "    for epoch in range(num_epochs):\n",
        "            validation_accuracy_scores = []\n",
        "            test_accuracy_scores = []\n",
        "            test_batch_scores = []\n",
        "            val_batch_scores = []\n",
        "            epoch_loss = 0\n",
        "            i=0\n",
        "            while i < num_minibatches: \n",
        "                print('mini batch'+str(i))\n",
        "                train_pickle_file = pickle_trainfiles_dir + 'train_minibatch_' + str(i)\n",
        "                with open(train_pickle_file, \"rb\") as fp:   # Unpickling\n",
        "                  batch = pickle.load(fp)\n",
        "                batch_x, batch_y = zip(*batch)\n",
        "\n",
        "                _, c = sess.run([train_op, loss], feed_dict={features_input: batch_x, labels_input: batch_y})\n",
        "                epoch_loss += c\n",
        "                i+=1\n",
        "            #print no. of epochs and loss\n",
        "            print('Epoch', epoch+1, 'completed out of', num_epochs,', loss:',epoch_loss) \n",
        "\n",
        "            #If these lines are left here, it will evaluate on the val and test data every iteration and print accuracy\n",
        "            #note this adds a small computational cost\n",
        "            correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_input, 1)) #This line returns the max value of each array, which we want to be the same (think the prediction/logits is value given to each class with the highest value being the best match)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct, 'float')) #changes correct to type: float\n",
        "            \n",
        "            ## Inferencing on validation data        \n",
        "            for z in range(num_val_batches):\n",
        "              val_pickle_file = pickle_valfiles_dir + 'val_minibatch_' + str(z)\n",
        "              with open(val_pickle_file, \"rb\") as fp:   # Unpickling\n",
        "                 val_batch = pickle.load(fp)\n",
        "              X_validation, y_validation = zip(*val_batch) # unzip the pickle output\n",
        "              validation_accuracy = accuracy.eval({features_input:X_validation, labels_input:y_validation}) #inference\n",
        "              val_batch_scores.append(validation_accuracy) #save accuracy score\n",
        "\n",
        "            val_avg_score = Average(val_batch_scores) # gets the average across all val minibatches for this epoch\n",
        "            validation_accuracy_scores.append(val_avg_score) # saves this average across val minibatches for the epoch\n",
        "            print('Validation accuracy:', val_avg_score) #TF is smart so just knows to feed it through the model without us seeming to tell it to. .eval() uses the current session which I guess is my model?\n",
        "            \n",
        "            if val_avg_score > highest_acc_score:\n",
        "              # If the validation accuracy improved, inferencing will be done on the test data\n",
        "              highest_acc_score = val_avg_score\n",
        "              best_epoch = str(epoch+1)\n",
        "              test_predictions = pd.DataFrame(columns = col_names)\n",
        "\n",
        "              ## Inferencing on test data        \n",
        "              for v in range(num_test_batches):\n",
        "                test_pickle_file = pickle_testfiles_dir + 'test_minibatch_' + str(v)\n",
        "                with open(test_pickle_file, \"rb\") as fp:   # Unpickling\n",
        "                  test_batch = pickle.load(fp)\n",
        "                X_test, y_test = zip(*test_batch) # unzip the pickle output                  \n",
        "                test_accuracy = accuracy.eval({features_input:X_test, labels_input:y_test}) \n",
        "                test_batch_scores.append(test_accuracy) #change this line to get avg\n",
        "                \n",
        "                #Save dataframe of predictions for test data\n",
        "                predictions_sigm = logits.eval(feed_dict = {features_input:X_test}) #get predictions from the test data features\n",
        "                temp_df = pd.DataFrame(predictions_sigm, columns = col_names[:-1]) #put these in a temp dataframe\n",
        "                true_class = np.argmax(y_test, axis = 1)     #This saves the true class from test data labels\n",
        "                temp_df['True class'] = true_class        #This adds true class to the temp_df\n",
        "                test_predictions = test_predictions.append(temp_df, ignore_index = True)          #append the temp df to the full df     ##############\n",
        "                #print(test_pred.shape)\n",
        "                #save as a csv for each epoch\n",
        "              \n",
        "              # Get test accuracy for this epoch\n",
        "              test_avg_score = Average(test_batch_scores) # gets the average across all val minibatches for this epoch\n",
        "              test_accuracy_scores.append(test_avg_score) # appends this average to a list containing the avg for each epoch\n",
        "              print('New validation accuracy benchmark, test accuracy:', test_avg_score)#accuracy.eval({features_input:X_test, labels_input:y_test})) #TF is smart so just knows to feed it through the model without us seeming to tell it to. .eval() uses the current session which I guess is my model?\n",
        "              #print(test_predictions)\n",
        "              # Save test predictions for this epoch\n",
        "              # names file with validation accuracy, not test accuracy. Final test accuracy should be determine by taking the mode prediction per min.\n",
        "            else:\n",
        "              # If validation accuracy did not improve\n",
        "              print('No validation accuracy improvement, test accuracy still: ' + str(test_avg_score))  \n",
        "    \n",
        "    # Save test data predictions for the epoch which had the highest val data accuracy\n",
        "    np.savetxt(results_dir + \"CrossValRepeat\" + str(repeat) + \"_Epoch\" + best_epoch + \"_ValAcc_\" + str(round(highest_acc_score, 5)) + \".csv\",\n",
        "                test_predictions, delimiter = \",\") #put 'r\"r'C:\\Users\\...\\test_predictions' to save in a different folder\n",
        "    print('Highest validation accuracy: ' + str(highest_acc_score))\n",
        "\n",
        "tf.app.run(main)   "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNJ59U9c141uC0UaopSWeUe",
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
