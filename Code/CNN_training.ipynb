{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenUCL/Reef-acoustics-and-AI/blob/main/Code/CNN_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train the CNN**\n",
        "\n",
        "This script provides an example of training the CNN on the minibatch files which can be created with the 'CNN minibatch creation' script. This uses a small subset of the Indonesian dataset.\n",
        "\n",
        "This outputs a csv file of predictions for each 0.96sec chunk from each audio files. These can be converted into predictions for whole minutes and a final test accuracy reported using the 'Trained CNN accuracy calculator' script - note csv's from the full datasets are provided from here on as intensive analysis with audio files is no longer needed.\n",
        "\n",
        "# **Using Colabs free GPU feature**\n",
        "\n",
        "Google colab provides free GPU access (with some limits), see here: https://research.google.com/colaboratory/faq.html\n",
        "\n",
        "This can be used to significantly increase training speed. To switch this on go to 'Runtime' at the top and change type to 'GPU'."
      ],
      "metadata": {
        "id": "WLRcmHXBrd50"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFSJ2V4TmVSq",
        "outputId": "823bc829-ff7e-45d2-db14-75b5d636440e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Connect your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h2jq6cOCT7Ca",
        "outputId": "941309fc-d47a-42d3-bc3a-28564804d2dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.21.5\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 38.8 MB/s \n",
            "\u001b[?25hCollecting resampy==0.2.2\n",
            "  Downloading resampy-0.2.2.tar.gz (323 kB)\n",
            "\u001b[K     |████████████████████████████████| 323 kB 66.7 MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.15\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 24 kB/s \n",
            "\u001b[?25hCollecting tf_slim==1.1.0\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 66.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
            "Collecting soundfile==0.10.3.post1\n",
            "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: scipy>=0.13 in /usr/local/lib/python3.7/dist-packages (from resampy==0.2.2) (1.7.3)\n",
            "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.7/dist-packages (from resampy==0.2.2) (0.56.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.37.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.49.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 61.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.14.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (2.0.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 52.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile==0.10.3.post1) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile==0.10.3.post1) (2.21)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.32->resampy==0.2.2) (5.0.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.32->resampy==0.2.2) (0.39.1)\n",
            "Requirement already satisfied: setuptools<60 in /usr/local/lib/python3.7/dist-packages (from numba>=0.32->resampy==0.2.2) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.32->resampy==0.2.2) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.32->resampy==0.2.2) (3.9.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
            "Building wheels for collected packages: resampy, gast\n",
            "  Building wheel for resampy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320732 sha256=232a8d99227cc565dc002a942786d3bd0132535aff8aa631c90757080adcbd95\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/18/0a/8ad18a597d8333a142c9789338a96a6208f1198d290ece356c\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=58b853bb969455dbf960df7097b3f2956c2030875bf5deec49ec63f19bcf0074\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built resampy gast\n",
            "Installing collected packages: numpy, tensorflow-estimator, tensorboard, keras-applications, gast, tf-slim, tensorflow, soundfile, resampy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "  Attempting uninstall: soundfile\n",
            "    Found existing installation: soundfile 0.11.0\n",
            "    Uninstalling soundfile-0.11.0:\n",
            "      Successfully uninstalled soundfile-0.11.0\n",
            "  Attempting uninstall: resampy\n",
            "    Found existing installation: resampy 0.4.2\n",
            "    Uninstalling resampy-0.4.2:\n",
            "      Successfully uninstalled resampy-0.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 numpy-1.21.5 resampy-0.2.2 soundfile-0.10.3.post1 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 tf-slim-1.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" As package versions began updating this threw errors on the smoke test. \\nFor a faster download versions could be removed but this may throw errors. \\nAs of 17/10/22 it gives the below output, but, the smoketest codeblock passes:\\n\\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\\ntensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\\nkapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\\nSuccessfully installed gast-0.2.2 keras-applications-1.0.8 llvmlite-0.32.1 numba-0.49.1 numpy-1.21.5 resampy-0.2.2 soundfile-0.10.3.post1 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 tf-slim-1.1.0\\nWARNING: The following packages were previously imported in this runtime:\\n  [numpy]\\nYou must restart the runtime in order to use newly installed versions. \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "!pip install numpy==1.21.5 resampy==0.2.2 tensorflow==1.15 tf_slim==1.1.0 six==1.15.0 soundfile==0.10.3.post1\n",
        "\n",
        "\"\"\" As package versions began updating this threw errors on the smoke test. \n",
        "For a faster download versions could be removed but this may throw errors. \n",
        "As of 17/10/22 it gives the below output, but, the smoketest codeblock passes:\n",
        "\n",
        "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
        "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
        "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\n",
        "Successfully installed gast-0.2.2 keras-applications-1.0.8 llvmlite-0.32.1 numba-0.49.1 numpy-1.21.5 resampy-0.2.2 soundfile-0.10.3.post1 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 tf-slim-1.1.0\n",
        "WARNING: The following packages were previously imported in this runtime:\n",
        "  [numpy]\n",
        "You must restart the runtime in order to use newly installed versions. \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV91rjp5T7KW",
        "outputId": "6e2297be-81a7-49fb-b292-d20b0be8b84d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Reef soundscapes with AI/Audioset\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "\n",
            "Testing your install of VGGish\n",
            "\n",
            "Log Mel Spectrogram example:  [[-4.47297436 -4.29457354 -4.14940631 ... -3.9747003  -3.94774997\n",
            "  -3.78687669]\n",
            " [-4.48589533 -4.28825497 -4.139964   ... -3.98368686 -3.94976505\n",
            "  -3.7951698 ]\n",
            " [-4.46158065 -4.29329706 -4.14905953 ... -3.96442484 -3.94895483\n",
            "  -3.78619839]\n",
            " ...\n",
            " [-4.46152626 -4.29365061 -4.14848608 ... -3.96638113 -3.95057575\n",
            "  -3.78538167]\n",
            " [-4.46152595 -4.2936572  -4.14848104 ... -3.96640507 -3.95059567\n",
            "  -3.78537143]\n",
            " [-4.46152565 -4.29366386 -4.14847603 ... -3.96642906 -3.95061564\n",
            "  -3.78536116]]\n",
            "2022-10-18 11:30:20.255226: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2022-10-18 11:30:20.327965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-10-18 11:30:20.328768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2022-10-18 11:30:20.329058: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-18 11:30:20.329234: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-18 11:30:20.330025: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-18 11:30:20.330932: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-18 11:30:20.331119: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-18 11:30:20.331866: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-18 11:30:20.332047: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-18 11:30:20.332069: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2022-10-18 11:30:20.332452: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "2022-10-18 11:30:20.339044: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000170000 Hz\n",
            "2022-10-18 11:30:20.339219: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x36824c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-10-18 11:30:20.339242: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2022-10-18 11:30:20.476368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-10-18 11:30:20.477238: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3682680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2022-10-18 11:30:20.477273: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2022-10-18 11:30:20.477404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2022-10-18 11:30:20.477427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1666: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "VGGish embedding:  [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.1613729  0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.806958\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.36792767 0.03582427 0.         0.         0.\n",
            " 0.         0.38027033 0.13755946 0.91747075 0.8065634  0.\n",
            " 0.         0.         0.         0.04036263 0.70762444 0.\n",
            " 0.49783885 0.24081798 0.21565425 0.88492286 1.1956799  0.67061967\n",
            " 0.2077946  0.0163987  0.17471863 0.         0.         0.25100812\n",
            " 0.         0.         0.14607906 0.         0.39887047 0.30542105\n",
            " 0.12896754 0.         0.         0.         0.         0.\n",
            " 0.5385133  0.         0.         0.0494107  0.42527398 0.18537286\n",
            " 0.         0.         0.14753525 0.         0.         0.6993386\n",
            " 0.45541197 0.05174825 0.         0.01992548 0.         0.\n",
            " 0.51815766 0.5655759  0.6587973  0.         0.         0.41056326\n",
            " 0.         0.         0.         0.2576519  0.23232117 0.24026442\n",
            " 0.         0.         0.         0.         0.         0.2652376\n",
            " 0.         0.48460817 0.         0.         0.19325797 0.\n",
            " 0.20123352 0.         0.03368608 0.         0.         0.\n",
            " 0.         0.17836356 0.02474898 0.06889964 0.         0.\n",
            " 0.         0.08246288 0.         0.         0.         0.\n",
            " 0.         0.        ]\n",
            "\n",
            "Looks Good To Me!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Should output 'Looks good to me at the bottom!'\n",
        "%cd /content/drive/MyDrive/Reef soundscapes with AI/Audioset\n",
        "!python vggish_smoke_test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4ZdEHlqrT7M6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a054435-9abe-4ba5-e200-5c2a3ba08e0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "#From original vggish_train_demo.py script on github\n",
        "from __future__ import print_function\n",
        "\n",
        "from random import shuffle\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tf_slim as slim\n",
        "\n",
        "import vggish_input\n",
        "import vggish_params\n",
        "import vggish_slim\n",
        "\n",
        "#Modules added by Ben\n",
        "import os #for handling directories\n",
        "import glob #for dealing with files in dir\n",
        "import pandas as pd #for saving output at end in dataframe\n",
        "import sklearn\n",
        "import math\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split #added for train/test split\n",
        "from numpy import loadtxt #addded so predictions can be output to CSV file\n",
        "from datetime import datetime #added to append time to csv output file name to prevent overwriting"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set paths to access modules and pickle files, also set CNN parameters.**\n",
        "\n",
        "Two classes are used here, increase _NUM_CLASSES if needed. A batch size of 16 was used as larger batches can cause a memory error on colab depending on which GPU you are  allocated. The network trains for 5 epochs currently to save computation time, the final study used UCL's computing cluster to train for 50 epochs on the full datasets."
      ],
      "metadata": {
        "id": "TTz1EZxtiRa9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0zN9KAOcWziG"
      },
      "outputs": [],
      "source": [
        "#which repeat of the cross-val is this? (1-8):\n",
        "repeat = 1 # Used to set seed for train/val/test split\n",
        "\n",
        "### Change paths if you re-structure folders\n",
        "\n",
        "# Path to the location where your audio file are stored:\n",
        "audio_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/audio_dir' \n",
        "\n",
        "# Path to folder containing vggish setup files and 'AudiosetAnalysis' downloaded from sarebs supplementary\n",
        "vggish_files = r'/content/drive/MyDrive/Reef soundscapes with AI/Audioset' \n",
        "\n",
        "# Output folder for results:\n",
        "results_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/Results/Colab_CNN_predictions/' \n",
        "\n",
        "#Set the directories where logmel-spectrograms will be stored for train, test and validation sets:\n",
        "pickle_trainfiles_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/Results/minibatches_train/'\n",
        "pickle_valfiles_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/Results/minibatches_val/'\n",
        "pickle_testfiles_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/Results/minibatches_test/'\n",
        "\n",
        "#how many classes?:\n",
        "_NUM_CLASSES = 2\n",
        "\n",
        "#name a column for each class e.g 'class1', 'class2', or 'healthy', 'degraded'\n",
        "col_names = 'Healthy','Degraded', 'True class'\n",
        "\n",
        "#Batch size:\n",
        "batch_size = 16 # larger batches can cause a memory error on the NN script on colab depending on which GPU you are  allocated \n",
        "\n",
        "# Number of epochs.\n",
        "num_epochs = 5\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Some final set up\n",
        "# Find number of minibatches for networks for loop\n",
        "minibatches = [filename for filename in os.listdir(pickle_trainfiles_dir) if filename.startswith(\"train_minibatch\")]\n",
        "num_minibatches = len(minibatches) #this takes the last digit of the last pickle files, denoting how many minibatches there are\n",
        "\n",
        "# Get number of train/test/val minibatches\n",
        "num_train_batches = int(len(os.listdir(pickle_trainfiles_dir)))\n",
        "print('Number of train minibatches found: ' + str(num_train_batches))\n",
        "num_val_batches = int(len(os.listdir(pickle_valfiles_dir)))\n",
        "print('Number of validation minibatches found: ' + str(num_val_batches))\n",
        "num_test_batches = int(len(os.listdir(pickle_testfiles_dir)))\n",
        "print('Number of test minibatches found: ' + str(num_test_batches))\n",
        "\n",
        "os.chdir(vggish_files) \n",
        "\n",
        "# Used to find averages of accuracy score across minibatches later\n",
        "def Average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "\n",
        "print('Cross validation combination: ' + str(repeat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apTYZYvDnbZN",
        "outputId": "0db24eb4-aa57-45fe-e648-b3958602458f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train minibatches found: 1\n",
            "Number of validation minibatches found: 1\n",
            "Number of test minibatches found: 1\n",
            "Cross validation combination: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thPSDwZmk2Dv"
      },
      "source": [
        "# **Run the neural network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fZuMo8UDkI55"
      },
      "outputs": [],
      "source": [
        "#RUN THIS BLOCK ONLY ONCE PER SESSION - otherwise it will error\n",
        "\n",
        "flags = tf.app.flags\n",
        "\n",
        "flags.DEFINE_boolean(\n",
        "    'train_vggish', True,\n",
        "    'If True, allow VGGish parameters to change during training, thus '\n",
        "    'fine-tuning VGGish. If False, VGGish parameters are fixed, thus using '\n",
        "    'VGGish as a fixed feature extractor.')\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    'checkpoint', 'vggish_model.ckpt',\n",
        "    'Path to the VGGish checkpoint file.')\n",
        "\n",
        "FLAGS = flags.FLAGS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'An exception has occurred, use %tb to see the full traceback.' error will occur, fear not, this just means its finished "
      ],
      "metadata": {
        "id": "dckDf8YewAX8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7E0YTlu4xNvt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "outputId": "6e55d82e-3954-4eb8-fbea-f08b71adf2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W1018 11:30:33.696923 140320038377344 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W1018 11:30:33.783624 140320038377344 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1666: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "W1018 11:30:33.858688 140320038377344 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "I1018 11:30:37.189760 140320038377344 saver.py:1284] Restoring parameters from vggish_model.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mini batch0\n",
            "Epoch 1 completed out of 2 , loss: 0.7162290811538696\n",
            "Validation accuracy: 0.5702764987945557\n",
            "New validation accuracy benchmark, test accuracy: 0.4000000059604645\n",
            "mini batch0\n",
            "Epoch 2 completed out of 2 , loss: 0.7042340636253357\n",
            "Validation accuracy: 0.5714285969734192\n",
            "New validation accuracy benchmark, test accuracy: 0.4000000059604645\n",
            "Highest validation accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "\"\"\"To train 5 epochs on the 123x1min files in the of training data this process\n",
        "takes up to 80 minutes on a CPU. Depending which GPU Colab provides you this can \n",
        "take < 5min on colabs GPU. The final study used NVIDIA A100 GPU's which \n",
        "generally provide the highest speed as of 2022.\"\"\"\n",
        "\n",
        "### Run the network and save the predictions and accuracy at each epoch\n",
        "\n",
        "#\n",
        "\n",
        "### Train NN, output results\n",
        "r\"\"\"This uses the VGGish model definition within a larger model which adds two \n",
        "layers on top, and then trains this larger model. \n",
        "\n",
        "We input log-mel spectrograms (X_train) calculated above with associated labels \n",
        "(y_train), and feed the batches into the model. Once the model is trained, it \n",
        "is then executed on the validation and log-mel spectrograms (X_validation, \n",
        "X_test), and the accuracy is output for each.\n",
        "\n",
        "Alongside .csv file with the predictions for each 0.96s chunk and their true\n",
        "class is also output for the test data. Column1 = the logit for the first class,\n",
        "Column2 = the logit for the scond class etc. The final column is the true class.\n",
        "\n",
        "Final accuracy is actually taken from these predictions in another script which \n",
        "takes the  most common predicted class across an entire minute using each 0.96s \n",
        "chunks prediction.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main(X):   \n",
        "  with tf.Graph().as_default(), tf.Session() as sess:\n",
        "    # Define VGGish.\n",
        "    embeddings = vggish_slim.define_vggish_slim(training=FLAGS.train_vggish)\n",
        "    \n",
        "    \n",
        "    # Define a shallow classification model and associated training ops on top\n",
        "    # of VGGish.\n",
        "    with tf.variable_scope('mymodel'):\n",
        "      # Add a fully connected layer with 100 units. Add an activation function\n",
        "      # to the embeddings since they are pre-activation.\n",
        "      num_units = 100\n",
        "      fc = slim.fully_connected(tf.nn.relu(embeddings), num_units)\n",
        "\n",
        "      # Add a classifier layer at the end, consisting of parallel logistic\n",
        "      # classifiers, one per class. This allows for multi-class tasks.\n",
        "    #  logits = slim.fully_connected(                                      ### logits threw me, would be easier to name this 'end model' or something\n",
        "     #     fc, _NUM_CLASSES, activation_fn=None, scope='logits')\n",
        "     # tf.sigmoid(logits, name='prediction')\n",
        "    \n",
        "      linear_out= slim.fully_connected(                                      \n",
        "          fc, _NUM_CLASSES, activation_fn=None, scope='linear_out')\n",
        "      logits = tf.sigmoid(linear_out, name='logits')\n",
        "    \n",
        "      # Add training ops.\n",
        "      with tf.variable_scope('train'):\n",
        "        global_step = tf.train.create_global_step()\n",
        "\n",
        "        # Labels are assumed to be fed as a batch multi-hot vectors, with\n",
        "        # a 1 in the position of each positive class label, and 0 elsewhere.\n",
        "        labels_input = tf.placeholder(\n",
        "            tf.float32, shape=(None, _NUM_CLASSES), name='labels')\n",
        "\n",
        "        # Cross-entropy label loss.\n",
        "        xent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=logits, labels=labels_input, name='xent')    ###=labels is selecting my 'y', logits is like a precursor to predictions?\n",
        "        loss = tf.reduce_mean(xent, name='loss_op')\n",
        "        tf.summary.scalar('loss', loss)\n",
        "\n",
        "        # We use the same optimizer and hyperparameters as used to train VGGish.\n",
        "        optimizer = tf.train.AdamOptimizer(\n",
        "            learning_rate=vggish_params.LEARNING_RATE,\n",
        "            epsilon=vggish_params.ADAM_EPSILON)\n",
        "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
        "\n",
        "    # Initialize all variables in the model, and then load the pre-trained\n",
        "    # VGGish checkpoint.\n",
        "    sess.run(tf.global_variables_initializer())         ### this starts the session appaz\n",
        "    vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n",
        "\n",
        "    \n",
        "    features_input = sess.graph.get_tensor_by_name(\n",
        "        vggish_params.INPUT_TENSOR_NAME)\n",
        "    \n",
        "    # The training loop.\n",
        "    #test_predictions = pd.DataFrame(columns = col_names) #this was in wrong place before\n",
        "    highest_acc_score = 0\n",
        "    for epoch in range(num_epochs):\n",
        "            validation_accuracy_scores = []\n",
        "            test_accuracy_scores = []\n",
        "            test_batch_scores = []\n",
        "            val_batch_scores = []\n",
        "            epoch_loss = 0\n",
        "            i=0\n",
        "            while i < num_minibatches: \n",
        "                print('mini batch'+str(i))\n",
        "                train_pickle_file = pickle_trainfiles_dir + 'train_minibatch_' + str(i)\n",
        "                with open(train_pickle_file, \"rb\") as fp:   # Unpickling\n",
        "                  batch = pickle.load(fp)\n",
        "                batch_x, batch_y = zip(*batch)\n",
        "\n",
        "                _, c = sess.run([train_op, loss], feed_dict={features_input: batch_x, labels_input: batch_y})\n",
        "                epoch_loss += c\n",
        "                i+=1\n",
        "            #print no. of epochs and loss\n",
        "            print('Epoch', epoch+1, 'completed out of', num_epochs,', loss:',epoch_loss) \n",
        "\n",
        "            #If these lines are left here, it will evaluate on the val and test data every iteration and print accuracy\n",
        "            #note this adds a small computational cost\n",
        "            correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_input, 1)) #This line returns the max value of each array, which we want to be the same (think the prediction/logits is value given to each class with the highest value being the best match)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct, 'float')) #changes correct to type: float\n",
        "            \n",
        "            ## Inferencing on validation data        \n",
        "            for z in range(num_val_batches):\n",
        "              val_pickle_file = pickle_valfiles_dir + 'val_minibatch_' + str(z)\n",
        "              with open(val_pickle_file, \"rb\") as fp:   # Unpickling\n",
        "                 val_batch = pickle.load(fp)\n",
        "              X_validation, y_validation = zip(*val_batch) # unzip the pickle output\n",
        "              validation_accuracy = accuracy.eval({features_input:X_validation, labels_input:y_validation}) #inference\n",
        "              val_batch_scores.append(validation_accuracy) #save accuracy score\n",
        "\n",
        "            val_avg_score = Average(val_batch_scores) # gets the average across all val minibatches for this epoch\n",
        "            validation_accuracy_scores.append(val_avg_score) # saves this average across val minibatches for the epoch\n",
        "            print('Validation accuracy:', val_avg_score)#accuracy.eval({features_input:X_test, labels_input:y_test})) #TF is smart so just knows to feed it through the model without us seeming to tell it to. .eval() uses the current session which I guess is my model?\n",
        "            \n",
        "            if val_avg_score > highest_acc_score:\n",
        "              # If the validation accuracy improved, inferencing will be done on the test data\n",
        "              highest_acc_score = val_avg_score\n",
        "              best_epoch = str(epoch+1)\n",
        "              test_predictions = pd.DataFrame(columns = col_names)\n",
        "\n",
        "              ## Inferencing on test data        \n",
        "              for v in range(num_test_batches):\n",
        "                test_pickle_file = pickle_testfiles_dir + 'test_minibatch_' + str(v)\n",
        "                with open(test_pickle_file, \"rb\") as fp:   # Unpickling\n",
        "                  test_batch = pickle.load(fp)\n",
        "                X_test, y_test = zip(*test_batch) # unzip the pickle output                  \n",
        "                test_accuracy = accuracy.eval({features_input:X_test, labels_input:y_test}) \n",
        "                test_batch_scores.append(test_accuracy) #change this line to get avg\n",
        "                \n",
        "                #Save dataframe of predictions for test data\n",
        "                predictions_sigm = logits.eval(feed_dict = {features_input:X_test}) #get predictions from the test data features\n",
        "                temp_df = pd.DataFrame(predictions_sigm, columns = col_names[:-1]) #put these in a temp dataframe\n",
        "                true_class = np.argmax(y_test, axis = 1)     #This saves the true class from test data labels\n",
        "                temp_df['True class'] = true_class        #This adds true class to the temp_df\n",
        "                test_predictions = test_predictions.append(temp_df, ignore_index = True)          #append the temp df to the full df     ##############\n",
        "                #print(test_pred.shape)\n",
        "                #save as a csv for each epoch\n",
        "              \n",
        "              # Get test accuracy for this epoch\n",
        "              test_avg_score = Average(test_batch_scores) # gets the average across all val minibatches for this epoch\n",
        "              test_accuracy_scores.append(test_avg_score) # appends this average to a list containing the avg for each epoch\n",
        "              print('New validation accuracy benchmark, test accuracy:', test_avg_score)#accuracy.eval({features_input:X_test, labels_input:y_test})) #TF is smart so just knows to feed it through the model without us seeming to tell it to. .eval() uses the current session which I guess is my model?\n",
        "              #print(test_predictions)\n",
        "              # Save test predictions for this epoch\n",
        "              # names file with validation accuracy, not test accuracy. Final test accuracy should be determine by taking the mode prediction per min.\n",
        "            else:\n",
        "              # If validation accuracy did not improve\n",
        "              print('No validation accuracy improvement, test accuracy still: ' + str(test_avg_score))  \n",
        "    \n",
        "    # Save test data predictions for the epoch which had the highest val data accuracy\n",
        "    np.savetxt(results_dir + \"CrossValRepeat\" + str(repeat) + \"_Epoch\" + best_epoch + \"_ValAcc_\" + str(round(highest_acc_score, 5)) + \".csv\",\n",
        "                test_predictions, delimiter = \",\") #put 'r\"r'C:\\Users\\...\\test_predictions' to save in a different folder\n",
        "    print('Highest validation accuracy: ' + str(highest_acc_score))\n",
        "\n",
        "tf.app.run(main)   "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyMkPG7LTC8rkJJYHQfUyt9y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}