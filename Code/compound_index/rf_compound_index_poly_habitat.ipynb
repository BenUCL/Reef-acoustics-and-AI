{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenUCL/Reef-acoustics-and-AI/blob/main/Code/Random_forests_compound_index_Poly_habitat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDy7x-Wqv0Ky"
      },
      "source": [
        "# **Random forest classifiers**\n",
        "\n",
        "This script splits the data into the same train/val/test sets as used for the pretrained CNN and trained CNN. It then trains on the training data and generates the accuracy for the validation data 50 times. The best performing model of the 50 repeats is used to inference on the test data and this accuracy is reported. This proces is repeated 100 times to generate an accuracyfor all train/val/test split combinations. A confusion matrix of all these is then generated.\n",
        "\n",
        "For polynesia site level classfication I take 1/4 days from each of the 8 sites at random as the val and test set. Every minute from this day is put in the test and every other in the val. So each of the test and val have recordings from all 8 sites. If i put 4 enitre sites in the val, and 4 in the test, then it would train to fit the val well, and do crap on the test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBo_8CMKMtvm",
        "outputId": "38fcb7c4-ba7e-43ff-c3d3-0bfc62a02466"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connect Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports, probably some redundant packages\n",
        "from __future__ import division\n",
        "\n",
        "import sklearn\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import statistics\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import collections\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload the csv of features and copy the path\n",
        "path = r'/content/drive/MyDrive/Reef soundscapes with AI/Results/full_dataset_features/compound_index_poly.csv'\n",
        "num_classes = 2\n",
        "labels = ['Photic', 'Mesophotic']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv (path)\n",
        "#Use the above functions to get a list of unique deployment ID's (approx 30 for healthy, and again for degraded)\n",
        "df = data.reset_index() \n",
        "df = df.iloc[: , 2:] #remove additional index columns that got added at start\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_class(filename):\n",
        "    #find part of the name that corresponds to the deployment\n",
        "     #adapted the get_identifier function above to only get class (e.g healthy)\n",
        "    site = filename[0:5]\n",
        "    return site\n",
        "    \n",
        "df_withclasses = df\n",
        "\n",
        "# add a column to the DF that contains class\n",
        "new_list = []\n",
        "for i in range(len(df_withclasses)):\n",
        "    new_list.append(get_class(df_withclasses['minute'][i]))\n",
        "\n",
        "df_withclasses.insert(1, 'Class', new_list)\n",
        "\n",
        "indices_df = df_withclasses \n",
        "\n",
        "# Rename the remaiming sites to mesophotic or photic fish\n",
        "indices_df['Class'] = indices_df['Class'].replace(['SiteA'],'Photic')\n",
        "indices_df['Class'] = indices_df['Class'].replace(['SiteB'],'Photic')\n",
        "indices_df['Class'] = indices_df['Class'].replace(['SiteC'],'Photic')\n",
        "indices_df['Class'] = indices_df['Class'].replace(['SiteD'],'Photic')\n",
        "\n",
        "indices_df['Class'] = indices_df['Class'].replace(['SiteW'],'Mesophotic')\n",
        "indices_df['Class'] = indices_df['Class'].replace(['SiteX'],'Mesophotic')\n",
        "indices_df['Class'] = indices_df['Class'].replace(['SiteY'],'Mesophotic')\n",
        "indices_df['Class'] = indices_df['Class'].replace(['SiteZ'],'Mesophotic')\n",
        "\n",
        "indices_df.reset_index(drop=True, inplace=True)\n",
        "indices_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def similar(x,y):\n",
        "    si = 0\n",
        "    for a,b in zip(x, y):\n",
        "        if a == b:\n",
        "            si += 1\n",
        "    return (si/len(x)) * 100\n",
        "\n",
        "def get_class(filename):\n",
        "    #find part of the name that corresponds to the deployment\n",
        "     #adapted the get_identifier function above to only get class (e.g healthy)\n",
        "    site = filename[0:5]\n",
        "    return site\n",
        "    \n",
        "df_withclasses = df\n",
        "\n",
        "# add a column to the DF that contains class\n",
        "new_list = []\n",
        "for i in range(len(df_withclasses)):\n",
        "    new_list.append(get_class(indices_df['minute'][i]))\n",
        "\n",
        "indices_df.insert(1, 'Site', new_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_saved_test_accs = []\n",
        "whole_site_saved_test_accs = []\n",
        "ConfusionMatrix = np.zeros((num_classes, num_classes), dtype=float)\n",
        "\n",
        "for i in range(4):  #4 sites in the photicfish class\n",
        "  photic = ['SiteA', 'SiteB', 'SiteC', 'SiteD'] #the 4 photic fish sites\n",
        "  remove_photic = photic[i] #select 1 to be the val site\n",
        "  train_photic = photic \n",
        "  train_photic.remove(remove_photic) #remove the val site to get the train photicfish sites\n",
        "  for k in range(4): #4 sites in the mesophoticfish class\n",
        "    thisCV_saved_test_accs = []\n",
        "    thisCV_saved_test_day_accs = []\n",
        "\n",
        "    mesophotic = ['SiteW', 'SiteX', 'SiteY', 'SiteZ'] #the 4 mesophotic fish sites\n",
        "    remove_mesophotic = mesophotic[k] #select 1 to be the test site\n",
        "    train_mesophotic = mesophotic \n",
        "    train_mesophotic.remove(remove_mesophotic) #remove the test site to get the train photicfish sites\n",
        "    \n",
        "    # Make the train df's, starting with photic fish then append mesophotic fish\n",
        "    train_df = indices_df.loc[indices_df['Site'].isin(train_photic)]\n",
        "    train_df = train_df.append(indices_df.loc[indices_df['Site'].isin(train_mesophotic)])\n",
        "\n",
        "    # create val df that is 50:50 photic and mesophotic fish\n",
        "    photicfish_df = indices_df.loc[indices_df['Site'] == remove_photic]\n",
        "    mesophoticfish_df = indices_df.loc[indices_df['Site'] == remove_mesophotic]\n",
        "    val_df = photicfish_df.iloc[::2,:] # take every other photicfish minute and create a new df\n",
        "    val_df = val_df.append(mesophoticfish_df.iloc[::2,:])\n",
        "    # create test df's that is the ramaining 50:50 photic and mesophotic fish\n",
        "    test_df = photicfish_df.iloc[1::2,:] # take every other photicfish minute and create a new df\n",
        "    test_df = test_df.append(mesophoticfish_df.iloc[1::2,:])\n",
        "\n",
        "    #shuffle the df's so the RF's aren't first fed all photic then fed all mesophotic fish after\n",
        "    np.random.seed(123) \n",
        "    train_df = train_df.sample(frac=1)\n",
        "    val_df = val_df.sample(frac=1)\n",
        "    test_df = test_df.sample(frac=1)\n",
        "    np.random.seed() #now lift the seed so that randomisation can be used again in the rest of the script\n",
        "\n",
        "    # Make new df's of the feats and labels to feed random forests\n",
        "    train_feats = train_df.iloc[:, 3:].to_numpy()\n",
        "    train_labels = train_df.iloc[:, 2].to_numpy()\n",
        "\n",
        "    val_feats = val_df.iloc[:, 3:].to_numpy()\n",
        "    val_labels = val_df.iloc[:, 2].to_numpy()\n",
        "\n",
        "    test_feats = test_df.iloc[:, 3:].to_numpy()\n",
        "    test_labels = test_df.iloc[:, 2].to_numpy()\n",
        "\n",
        "    \n",
        "    val_accuracy_score = 0\n",
        "    for k in range(50):              # Picked 50 as 50 epochs used in NN\n",
        "      #Training on train data\n",
        "      model = RandomForestClassifier(n_jobs = -1, random_state=k)\n",
        "      model.fit(train_feats, train_labels)\n",
        "\n",
        "      #Inferencing on validation data\n",
        "      new_val_acc = model.score(val_feats, val_labels)\n",
        "\n",
        "      #If val acc improved, inference on test data\n",
        "      if new_val_acc >  val_accuracy_score:\n",
        "        #save the new mesophotic score for val\n",
        "        val_accuracy_score = new_val_acc \n",
        "\n",
        "        #get 1min acc for test data\n",
        "        test_acc = model.score(test_feats, test_labels) \n",
        "        \n",
        "        #get the acc when using the most common prediction across all minutes in a day for each site\n",
        "        test_predictions = model.predict(test_feats)\n",
        "\n",
        "        # get confusion matrix values\n",
        "        best_ConfusionMatrix = confusion_matrix(test_labels, test_predictions, labels = labels) ###\n",
        "\n",
        "        #acc = 0 \n",
        "        #for i in range(num_classes):\n",
        "          # max = np.amax(best_ConfusionMatrix[i])\n",
        "          #if max == best_ConfusionMatrix[i,i]:\n",
        "            # acc += 1\n",
        "            #whole_day_acc = acc/num_classes\n",
        "        #print(similar(test_labels, test_predictions))\n",
        "        if  similar(test_labels, test_predictions) >= 50:\n",
        "          whole_site_acc = 1\n",
        "        else:\n",
        "          whole_site_acc = 0\n",
        "        \n",
        "      \n",
        "    #Save best results\n",
        "    ConfusionMatrix = np.add(ConfusionMatrix, best_ConfusionMatrix) ###\n",
        "    thisCV_saved_test_accs.append(test_acc)\n",
        "    all_saved_test_accs.append(test_acc)\n",
        "    thisCV_saved_test_day_accs.append(whole_site_acc)\n",
        "    whole_site_saved_test_accs.append(whole_site_acc)\n",
        "\n",
        "    print('Accuracies for 1min files from this site:')\n",
        "    print(thisCV_saved_test_accs)\n",
        "    print('Accuracies for the whole site [0 or 1]:')\n",
        "    print(thisCV_saved_test_day_accs)\n",
        "    thisCV_saved_test_accs = []\n",
        "    thisCV_saved_test_day_accs = []\n",
        "    print()\n",
        "\n",
        "\n",
        "pt1_saved_test_accs = all_saved_test_accs\n",
        "pt1_whole_site_saved_test_accs = whole_site_saved_test_accs\n",
        "\n",
        "\n",
        "# this will run all the lowfish sites as val, then repeat the whole thing below but swap the highfish to be in the top loop and low fish in bottom loop\n",
        "    \n",
        "    \n",
        "print('Part 1 Completed RFs:')\n",
        "print(len(all_saved_test_accs))\n",
        "\n",
        "def Average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "mean_accuracy = Average(all_saved_test_accs)\n",
        "stdev = np.std(all_saved_test_accs)\n",
        "\n",
        "print('1min saved_test_accs: ')\n",
        "print(all_saved_test_accs)\n",
        "result = 'Mean accuracy for 1min files with standard deviation = {} (±{})'.format(str(mean_accuracy), str(stdev))\n",
        "print(result)     \n",
        "\n",
        "\n",
        "print('Part 1 Completed RFs:')\n",
        "print(len(whole_site_saved_test_accs))\n",
        "\n",
        "def Average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "mean_accuracy = Average(whole_site_saved_test_accs)\n",
        "stdev = np.std(whole_site_saved_test_accs)\n",
        "\n",
        "print('Whole site saved test_accs: ')\n",
        "print(whole_site_saved_test_accs)\n",
        "result = 'Mean accuracy for whole sites with standard deviation = {} (±{})'.format(str(mean_accuracy), str(stdev))\n",
        "print(result)\n",
        "\n",
        "\n",
        "all_saved =  mean_accuracy\n",
        "stdev_pt1 = stdev\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "all_saved_test_accs = []\n",
        "whole_site_saved_test_accs = []\n",
        "#ConfusionMatrix = np.zeros((num_classes, num_classes), dtype=float)\n",
        "\n",
        "for i in range(4):  #4 sites in the photicfish class\n",
        "  mesophotic =  ['SiteA', 'SiteB', 'SiteC', 'SiteD'] #the 4 mesophotic fish sites\n",
        "  remove_mesophotic = mesophotic[i] #select 1 to be the val site\n",
        "  train_mesophotic = mesophotic \n",
        "  train_mesophotic.remove(remove_mesophotic) #remove the val site to get the train photicfish sites\n",
        "  #print(train_photic)\n",
        "  for k in range(4): #4 sites in the mesophoticfish class\n",
        "    thisCV_saved_test_accs = []\n",
        "    thisCV_saved_test_day_accs = []\n",
        "\n",
        "    photic = ['SiteW', 'SiteX', 'SiteY', 'SiteZ'] #the 4 photic fish sites\n",
        "    remove_photic = photic[k] #select 1 to be the test site\n",
        "    train_photic = photic \n",
        "    train_photic.remove(remove_photic) #remove the test site to get the train photicfish sites\n",
        "    \n",
        "    # Make the train df's, starting with photic fish then append mesophotic fish\n",
        "    train_df = indices_df.loc[indices_df['Site'].isin(train_photic)]\n",
        "    train_df = train_df.append(indices_df.loc[indices_df['Site'].isin(train_mesophotic)])\n",
        "\n",
        "    # create val df that is 50:50 photic and mesophotic fish\n",
        "    photicfish_df = indices_df.loc[indices_df['Site'] == remove_photic]\n",
        "    mesophoticfish_df = indices_df.loc[indices_df['Site'] == remove_mesophotic]\n",
        "    val_df = photicfish_df.iloc[::2,:] # take every other photicfish minute and create a new df\n",
        "    val_df = val_df.append(mesophoticfish_df.iloc[::2,:])\n",
        "    # create test df's that is the ramaining 50:50 photic and mesophotic fish\n",
        "    test_df = photicfish_df.iloc[1::2,:] # take every other photicfish minute and create a new df\n",
        "    test_df = test_df.append(mesophoticfish_df.iloc[1::2,:])\n",
        "\n",
        "    #shuffle the df's so the RF's aren't first fed all photic then fed all mesophotic fish after\n",
        "    np.random.seed(123) \n",
        "    train_df = train_df.sample(frac=1)\n",
        "    val_df = val_df.sample(frac=1)\n",
        "    test_df = test_df.sample(frac=1)\n",
        "    np.random.seed() #now lift the seed so that randomisation can be used again in the rest of the script\n",
        "\n",
        "    # Make new df's of the feats and labels to feed random forests\n",
        "    train_feats = train_df.iloc[:, 3:].to_numpy()\n",
        "    train_labels = train_df.iloc[:, 2].to_numpy()\n",
        "\n",
        "    val_feats = val_df.iloc[:, 3:].to_numpy()\n",
        "    val_labels = val_df.iloc[:, 2].to_numpy()\n",
        "\n",
        "    test_feats = test_df.iloc[:, 3:].to_numpy()\n",
        "    test_labels = test_df.iloc[:, 2].to_numpy()\n",
        "\n",
        "    \n",
        "    val_accuracy_score = 0\n",
        "    for k in range(50):              # Picked 50 as 50 epochs used in NN\n",
        "      #Training on train data\n",
        "      model = RandomForestClassifier(n_jobs = -1, random_state=k)\n",
        "      model.fit(train_feats, train_labels)\n",
        "\n",
        "      #Inferencing on validation data\n",
        "      new_val_acc = model.score(val_feats, val_labels)\n",
        "\n",
        "      #If val acc improved, inference on test data\n",
        "      if new_val_acc >  val_accuracy_score:\n",
        "        #save the new mesophotic score for val\n",
        "        val_accuracy_score = new_val_acc \n",
        "\n",
        "        #get 1min acc for test data\n",
        "        test_acc = model.score(test_feats, test_labels) \n",
        "        \n",
        "        #get the acc when using the most common prediction across all minutes in a day for each site\n",
        "        test_predictions = model.predict(test_feats)\n",
        "\n",
        "        # get confusion matrix values\n",
        "        best_ConfusionMatrix = confusion_matrix(test_labels, test_predictions, labels = labels) ###\n",
        "\n",
        "        if  similar(test_labels, test_predictions) >= 50:\n",
        "          whole_site_acc = 1\n",
        "        else:\n",
        "          whole_site_acc = 0\n",
        "        \n",
        "      \n",
        "    #Save best results\n",
        "    ConfusionMatrix = np.add(ConfusionMatrix, best_ConfusionMatrix) ###\n",
        "    thisCV_saved_test_accs.append(test_acc)\n",
        "    all_saved_test_accs.append(test_acc)\n",
        "    thisCV_saved_test_day_accs.append(whole_site_acc)\n",
        "    whole_site_saved_test_accs.append(whole_site_acc)\n",
        "\n",
        "    print('Accuracies for 1min files from this site:')\n",
        "    print(thisCV_saved_test_accs)\n",
        "    print('Accuracies for the whole site [0 or 1]:')\n",
        "    print(thisCV_saved_test_day_accs)\n",
        "    thisCV_saved_test_accs = []\n",
        "    thisCV_saved_test_day_accs = []\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "both_saved_test_accs = pt1_saved_test_accs + all_saved_test_accs \n",
        "both_whole_site_saved_test_accs = pt1_whole_site_saved_test_accs + whole_site_saved_test_accs\n",
        "    \n",
        "print('Part 2 Completed RFs:')\n",
        "print(len(both_saved_test_accs))\n",
        "\n",
        "def Average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "mean_accuracy = Average(both_saved_test_accs)\n",
        "stdev = np.std(both_saved_test_accs)\n",
        "\n",
        "print('1min saved_test_accs: ')\n",
        "print(both_saved_test_accs)\n",
        "result = 'Mean accuracy for 1min files with standard deviation = {} (±{})'.format(str(mean_accuracy), str(stdev))\n",
        "print(result) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Part 2 Completed RFs:')\n",
        "print(len(both_whole_site_saved_test_accs))\n",
        "\n",
        "def Average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "mean_accuracy = Average(both_whole_site_saved_test_accs)\n",
        "stdev = np.std(both_whole_site_saved_test_accs)\n",
        "\n",
        "print('Whole site saved test_accs: ')\n",
        "print(both_whole_site_saved_test_accs)\n",
        "result = 'Mean accuracy for whole sites with standard deviation = {} (±{})'.format(str(mean_accuracy), str(stdev))\n",
        "print(result) \n",
        "\n",
        "print()\n",
        "print(repr(ConfusionMatrix))\n",
        "\n",
        "\n",
        "\n",
        "executionTime = (time.time() - startTime)\n",
        "print('Execution time in seconds: ' + str(executionTime))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#HPConfusionMatrix = [[1.3010e+04, 8.4000e+01, 0.0000e+00, 4.0000e+00, 0.0000e+00,\n",
        "      #   2.3000e+01, 0.0000e+00, 2.0000e+00],\n",
        "      #  [2.0500e+02, 1.4088e+04, 0.0000e+00, 1.3000e+01, 0.0000e+00,\n",
        "      #   0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
        "      #  [0.0000e+00, 0.0000e+00, 1.4395e+04, 0.0000e+00, 2.0000e+00,\n",
        "      #   1.5000e+01, 2.0000e+01, 2.0000e+00],\n",
        "      #  [5.3000e+01, 7.0000e+00, 1.2000e+01, 1.4226e+04, 1.5000e+01,\n",
        "      #   1.9000e+01, 0.0000e+00, 4.9000e+01],\n",
        "      #  [0.0000e+00, 0.0000e+00, 4.0000e+00, 0.0000e+00, 1.3063e+04,\n",
        "      #   1.4100e+02, 8.0000e+00, 1.4000e+01],\n",
        "      #  [0.0000e+00, 0.0000e+00, 1.3000e+01, 0.0000e+00, 7.0000e+00,\n",
        "      #   1.4187e+04, 2.0000e+00, 5.3000e+01],\n",
        "      #  [0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.3000e+01,\n",
        "      #   1.6000e+01, 1.4200e+04, 1.5900e+02],\n",
        "      #  [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8000e+01,\n",
        "      #   4.2000e+01, 0.0000e+00, 1.4340e+04]]\n",
        "\n",
        "array = ConfusionMatrix\n",
        "df_cm = pd.DataFrame(array, index = [i for i in \"ABCDWXYZ\"],\n",
        "                  columns = [i for i in \"ABCDWXYZ\"])\n",
        "plt.figure(figsize = (15,12))\n",
        "cmap = sn.cm.rocket_r\n",
        "ax = sn.heatmap(df_cm, annot=True, annot_kws={\"fontsize\":20}, fmt='g', cmap = cmap)\n",
        "ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 20)\n",
        "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 20)\n",
        "cbar = ax.collections[0].colorbar\n",
        "cbar.ax.tick_params(labelsize=20)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPBZR+QxHAn1+JFqo0itp/+",
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
