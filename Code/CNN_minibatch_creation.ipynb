{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMfesPMuXmT7Ysz1DdmR+Ll",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenUCL/Reef-acoustics-and-AI/blob/main/Code/CNN_minibatch_creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create minibatches for CNN training**\n",
        "\n",
        "This scripts splits files into train, validation and training sets. This ensures that recordings from the same 1hr block from the Indo data are put into the same minibatch set. This prevents recordings from the same 1hr block being placed in training and test/val sets which may boost accuracy due to their close proximity in time.\n",
        "\n",
        "Audio files are then read and log-mel spectrogram matrices from them of size [n, 96, 64] are extracted, so that audio is split into 0.96sec chunks with 64 freq bins, where n = how many 0.96sec chunks can fit in the file (e.g 62 for 1min).\n",
        "\n",
        "These matrices are then put into pickle files and saved on your GDrive so that the CNN can access these. This is because: i) larger datasets would require too much memory if they were read in one go, ii) features do not need recalculating every time you return to the CNN training script.\n",
        "\n"
      ],
      "metadata": {
        "id": "IL3-ZPGCiu-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFSJ2V4TmVSq",
        "outputId": "d321d1fd-b9d7-48d6-a27a-75ac37e541bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the pretrained CNN (VGGish) and required packages. The smoke test code block should return 'Looks good to me!' right at the bottom."
      ],
      "metadata": {
        "id": "HxOC8AVIjqX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.21.5 resampy==0.2.2 tensorflow==1.15 tf_slim==1.1.0 six==1.15.0 soundfile==0.10.3.post1\n",
        "\n",
        "\"\"\" As package versions began updating this threw errors on the smoke test. \n",
        "For a faster download versions could be removed but this may throw errors. \n",
        "As of 17/10/22 it gives the below output, but, the smoketest codeblock passes:\n",
        "\n",
        "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
        "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
        "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\n",
        "Successfully installed gast-0.2.2 keras-applications-1.0.8 llvmlite-0.32.1 numba-0.49.1 numpy-1.21.5 resampy-0.2.2 soundfile-0.10.3.post1 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 tf-slim-1.1.0\n",
        "WARNING: The following packages were previously imported in this runtime:\n",
        "  [numpy]\n",
        "You must restart the runtime in order to use newly installed versions. \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h2jq6cOCT7Ca",
        "outputId": "b228dada-a852-40ea-99e4-9c4d4977906d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.21.5\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 6.5 MB/s \n",
            "\u001b[?25hCollecting resampy==0.2.2\n",
            "  Downloading resampy-0.2.2.tar.gz (323 kB)\n",
            "\u001b[K     |████████████████████████████████| 323 kB 46.7 MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.15\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 21 kB/s \n",
            "\u001b[?25hCollecting tf_slim==1.1.0\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 43.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
            "Collecting soundfile==0.10.3.post1\n",
            "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: scipy>=0.13 in /usr/local/lib/python3.7/dist-packages (from resampy==0.2.2) (1.7.3)\n",
            "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.7/dist-packages (from resampy==0.2.2) (0.56.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.49.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (2.0.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 61.7 MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 34.8 MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.2.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.14.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile==0.10.3.post1) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile==0.10.3.post1) (2.21)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: setuptools<60 in /usr/local/lib/python3.7/dist-packages (from numba>=0.32->resampy==0.2.2) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.32->resampy==0.2.2) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.32->resampy==0.2.2) (5.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.32->resampy==0.2.2) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.32->resampy==0.2.2) (3.9.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
            "Building wheels for collected packages: resampy, gast\n",
            "  Building wheel for resampy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320732 sha256=81ebf6e5a0d3d757063c198972d98de0fb44defb904d9aa6cef5a820d49687ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/18/0a/8ad18a597d8333a142c9789338a96a6208f1198d290ece356c\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=8604f135c5f4c56328cbc818500047105a1f9f497daa8dc10511d54767adfcf3\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built resampy gast\n",
            "Installing collected packages: numpy, tensorflow-estimator, tensorboard, keras-applications, gast, tf-slim, tensorflow, soundfile, resampy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "  Attempting uninstall: soundfile\n",
            "    Found existing installation: soundfile 0.11.0\n",
            "    Uninstalling soundfile-0.11.0:\n",
            "      Successfully uninstalled soundfile-0.11.0\n",
            "  Attempting uninstall: resampy\n",
            "    Found existing installation: resampy 0.4.2\n",
            "    Uninstalling resampy-0.4.2:\n",
            "      Successfully uninstalled resampy-0.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 numpy-1.21.5 resampy-0.2.2 soundfile-0.10.3.post1 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 tf-slim-1.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" As package versions began updating this threw errors on the smoke test. \\nFor a faster download versions could be removed but this may throw errors. \\nAs of 17/10/22 it gives the below output, but, the smoketest codeblock passes:\\n\\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\\ntensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\\nkapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\\nSuccessfully installed gast-0.2.2 keras-applications-1.0.8 llvmlite-0.32.1 numba-0.49.1 numpy-1.21.5 resampy-0.2.2 soundfile-0.10.3.post1 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 tf-slim-1.1.0\\nWARNING: The following packages were previously imported in this runtime:\\n  [numpy]\\nYou must restart the runtime in order to use newly installed versions. \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Should output 'Looks good to me at the bottom!'\n",
        "%cd /content/drive/MyDrive/Reef soundscapes with AI/Audioset\n",
        "!python vggish_smoke_test.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV91rjp5T7KW",
        "outputId": "1ba4be38-63db-443b-dc9e-68d123378a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Reef soundscapes with AI/Audioset\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "\n",
            "Testing your install of VGGish\n",
            "\n",
            "Log Mel Spectrogram example:  [[-4.47297436 -4.29457354 -4.14940631 ... -3.9747003  -3.94774997\n",
            "  -3.78687669]\n",
            " [-4.48589533 -4.28825497 -4.139964   ... -3.98368686 -3.94976505\n",
            "  -3.7951698 ]\n",
            " [-4.46158065 -4.29329706 -4.14905953 ... -3.96442484 -3.94895483\n",
            "  -3.78619839]\n",
            " ...\n",
            " [-4.46152626 -4.29365061 -4.14848608 ... -3.96638113 -3.95057575\n",
            "  -3.78538167]\n",
            " [-4.46152595 -4.2936572  -4.14848104 ... -3.96640507 -3.95059567\n",
            "  -3.78537143]\n",
            " [-4.46152565 -4.29366386 -4.14847603 ... -3.96642906 -3.95061564\n",
            "  -3.78536116]]\n",
            "2022-10-18 09:49:27.307407: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2022-10-18 09:49:27.382342: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2022-10-18 09:49:27.382428: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (7df935bba541): /proc/driver/nvidia/version does not exist\n",
            "2022-10-18 09:49:27.383038: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-10-18 09:49:27.390541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200220000 Hz\n",
            "2022-10-18 09:49:27.390789: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x426d500 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-10-18 09:49:27.390818: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1666: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "VGGish embedding:  [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.16137294 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.80695784\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.36792755 0.03582418 0.         0.         0.\n",
            " 0.         0.38027036 0.13755938 0.9174708  0.80656356 0.\n",
            " 0.         0.         0.         0.04036269 0.7076244  0.\n",
            " 0.4978391  0.24081807 0.21565425 0.884923   1.1956801  0.67061985\n",
            " 0.2077946  0.01639876 0.17471859 0.         0.         0.25100812\n",
            " 0.         0.         0.14607906 0.         0.39887053 0.30542108\n",
            " 0.1289675  0.         0.         0.         0.         0.\n",
            " 0.5385135  0.         0.         0.04941082 0.42527413 0.18537286\n",
            " 0.         0.         0.1475353  0.         0.         0.6993387\n",
            " 0.45541185 0.05174828 0.         0.01992539 0.         0.\n",
            " 0.5181578  0.56557596 0.6587975  0.         0.         0.41056335\n",
            " 0.         0.         0.         0.25765198 0.23232105 0.24026453\n",
            " 0.         0.         0.         0.         0.         0.2652375\n",
            " 0.         0.48460817 0.         0.         0.19325784 0.\n",
            " 0.20123352 0.         0.03368617 0.         0.         0.\n",
            " 0.         0.17836353 0.024749   0.06889973 0.         0.\n",
            " 0.         0.08246295 0.         0.         0.         0.\n",
            " 0.         0.        ]\n",
            "\n",
            "Looks Good To Me!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "CTJoexWWiOO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# May be some redundancy here\n",
        "from __future__ import print_function\n",
        "import random\n",
        "from random import shuffle\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tf_slim as slim\n",
        "\n",
        "import vggish_input\n",
        "import vggish_params\n",
        "import vggish_slim\n",
        "\n",
        "#Modules added by Ben\n",
        "import os #for handling directories\n",
        "import glob #for dealing with files in dir\n",
        "import pandas as pd #for saving output at end in dataframe\n",
        "import sklearn #added for train/test split\n",
        "from sklearn.model_selection import train_test_split #added for train/test split\n",
        "from numpy import loadtxt #addded so predictions can be output to CSV file\n",
        "from datetime import datetime #added to append time to csv output file name to prevent overwriting\n",
        "import pickle #for storing minibatches in pickle files"
      ],
      "metadata": {
        "id": "4ZdEHlqrT7M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set paths to access modules and where to store pickle files.**\n",
        "\n",
        "Also set the number of classes and their names, note codeblocks further down\n",
        "will need changing where highlighted with ##### if more classes are added"
      ],
      "metadata": {
        "id": "TTz1EZxtiRa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#which repeat of the cross-val is this? (1-8):\n",
        "repeat = 1 # Used to set seed for train/val/test split\n",
        "\n",
        "### Change paths if you re-structure folders\n",
        "\n",
        "# Path to the location where your audio file are stored:\n",
        "audio_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/audio_dir' \n",
        "\n",
        "# Path to folder containing vggish setup files and 'AudiosetAnalysis' downloaded from sarebs supplementary\n",
        "vggish_files = r'/content/drive/MyDrive/Reef soundscapes with AI/Audioset' \n",
        "\n",
        "# Output folder for results:\n",
        "results_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/Results/' \n",
        "\n",
        "#Set the directories where logmel-spectrograms will be stored for train, test and validation sets:\n",
        "pickle_trainfiles_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/Results/minibatches_train'\n",
        "pickle_valfiles_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/Results/minibatches_val'\n",
        "pickle_testfiles_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/Results/minibatches_test'\n",
        "\n",
        "#how many classes?:\n",
        "_NUM_CLASSES = 2\n",
        "\n",
        "#name a column for each class e.g 'class1', 'class2', or 'healthy', 'degraded'\n",
        "col_names = 'Healthy','Degraded'\n",
        "\n",
        "#Batch size:\n",
        "batch_size = 16 # larger batches can cause a memory error on the NN script on colab depending on which GPU you are  allocated "
      ],
      "metadata": {
        "id": "0zN9KAOcWziG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check how many files are in the directory (should be 152)**"
      ],
      "metadata": {
        "id": "abVxyAmgc8nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(audio_dir)\n",
        "print(len([name for name in os.listdir('.') if os.path.isfile(name)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-tfuC_6mTtc",
        "outputId": "9ebbf813-c535-4f83-ca72-4f3309a3bd21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select files for training, test and validation sets**"
      ],
      "metadata": {
        "id": "I-aiBHV5Wscq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''This block finds the unique identifiers of each deployment (i.e what\n",
        "hour of the day at what site) and splits these ID's into training, val and\n",
        "test sets. These are used in the next codeblock to select the actual \n",
        "recordings using these ID's which are present within each minute from the\n",
        "same deployment'''\n",
        "\n",
        "#This function takes the parts of a filename that make it unique\n",
        " #This uses Tims naming convention, specific to the 2018 Indonesia data\n",
        "def get_identifier(filename):\n",
        "    #find part of the name that corresponds to the deployment\n",
        "    t0 = filename.split(\".\")[0]\n",
        "    t1 = filename.split(\".\")[1][0:5]\n",
        "    t = t0+'.'+t1\n",
        "    return t\n",
        "\n",
        " \n",
        "#Function to get unique values within an array\n",
        "def unique(list1):\n",
        "    x = np.array(list1)\n",
        "    return np.unique(x)\n",
        "\n",
        "    \n",
        "#Get deployment ID from each file\n",
        " #For indo, deployments were made in 1hr blocks so will include 60*1min files for each deployment\n",
        "os.chdir(audio_dir)\n",
        "all_files = []\n",
        "all_IDs = []\n",
        "for file in glob.glob(\"*.wav\"):\n",
        "  all_files.append(file)\n",
        "  all_IDs.append(get_identifier(file))\n",
        "\n",
        "#Use the above function to get a list of unique deployment ID's (approx 30 for healthy, and again for degraded)\n",
        "unshuffled_unique_deployments = unique(all_IDs) #so for the real data this will give a big long order list\n",
        "\n",
        "#shuffle this list\n",
        "np.random.seed(123) #this ensures the same random shuffle is made each time, so the order is conserved when resubmitted\n",
        "shuffled_unique_deployments = np.random.permutation(unshuffled_unique_deployments)\n",
        "np.random.seed() #now lift the seed so that randomisation can be used again in the rest of the script\n",
        "\n",
        "print('Number of unique deployments:')\n",
        "print(len(shuffled_unique_deployments))\n",
        "print('The shuffled list of IDs corresponding to these:')\n",
        "print(shuffled_unique_deployments)\n",
        "\n",
        "#Bin deployments to the train, val and test sets\n",
        "a = (repeat*6)-6\n",
        "b = repeat*6\n",
        "c = (repeat*6)+6\n",
        "d = len(shuffled_unique_deployments)\n",
        "val_deployments =   shuffled_unique_deployments[a:b]\n",
        "test_deployments = shuffled_unique_deployments[b:c]\n",
        "train_deployments = np.concatenate((shuffled_unique_deployments[c:d],shuffled_unique_deployments[0:a]))\n",
        "print('IDs designated to the validation set:')\n",
        "print(val_deployments)\n",
        "print('IDs designated to the test set:')\n",
        "print(test_deployments)\n",
        "print('IDs designated to the training set:')\n",
        "print(train_deployments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAsGPFMMmb3U",
        "outputId": "a02c91c2-4522-4940-cb3b-5350e17f0b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique deployments:\n",
            "57\n",
            "The shuffled list of IDs corresponding to these:\n",
            "['BoN12.1220D' 'BoN11.1200H' 'BaN12.0529H' 'SaF4.0533D' 'BoF5.0532D'\n",
            " 'BoF4.2333H' 'SaF5.1202D' 'BoN11.1200D' 'BoF2.0930H' 'BoF4.1733D'\n",
            " 'BoF5.0940H' 'BoF5.0940D' 'BaN11.1731H' 'BaF1.1055H' 'BaF4.1040H'\n",
            " 'BaN11.1315H' 'SaF3.1733D' 'BaF5.2332H' 'BaN11.2330H' 'SaF3.1355D'\n",
            " 'SaF4.0902D' 'BoF4.1300D' 'BaF2.1733H' 'BaF5.1330H' 'BoN10.1731D'\n",
            " 'BoN11.0529D' 'SaN11.0940D' 'BaF5.1732H' 'BoN11.1103D' 'BoF4.1733H'\n",
            " 'BoF1.1315D' 'SaF2.1203D' 'SaF5.1125D' 'BoF3.1205H' 'SaN10.1345D'\n",
            " 'BaF3.0533H' 'BaF3.0915H' 'BoF1.1328H' 'SaF1.1733D' 'BoN11.0529H'\n",
            " 'BaN12.0915H' 'SaF3.2333D' 'BoF4.2333D' 'SaF1.2333D' 'SaF2.1112D'\n",
            " 'BaN10.0927H' 'BoN10.1731H' 'BoN10.2359D' 'BoF4.1300H' 'BoN12.1220H'\n",
            " 'BoF3.1205D' 'BoF2.0930D' 'BoN11.1103H' 'BoN10.2359H' 'BoF5.0532H'\n",
            " 'BaF2.2333H' 'SaF2.0534D']\n",
            "IDs designated to the validation set:\n",
            "['BoN12.1220D' 'BoN11.1200H' 'BaN12.0529H' 'SaF4.0533D' 'BoF5.0532D'\n",
            " 'BoF4.2333H']\n",
            "IDs designated to the test set:\n",
            "['SaF5.1202D' 'BoN11.1200D' 'BoF2.0930H' 'BoF4.1733D' 'BoF5.0940H'\n",
            " 'BoF5.0940D']\n",
            "IDs designated to the training set:\n",
            "['BaN11.1731H' 'BaF1.1055H' 'BaF4.1040H' 'BaN11.1315H' 'SaF3.1733D'\n",
            " 'BaF5.2332H' 'BaN11.2330H' 'SaF3.1355D' 'SaF4.0902D' 'BoF4.1300D'\n",
            " 'BaF2.1733H' 'BaF5.1330H' 'BoN10.1731D' 'BoN11.0529D' 'SaN11.0940D'\n",
            " 'BaF5.1732H' 'BoN11.1103D' 'BoF4.1733H' 'BoF1.1315D' 'SaF2.1203D'\n",
            " 'SaF5.1125D' 'BoF3.1205H' 'SaN10.1345D' 'BaF3.0533H' 'BaF3.0915H'\n",
            " 'BoF1.1328H' 'SaF1.1733D' 'BoN11.0529H' 'BaN12.0915H' 'SaF3.2333D'\n",
            " 'BoF4.2333D' 'SaF1.2333D' 'SaF2.1112D' 'BaN10.0927H' 'BoN10.1731H'\n",
            " 'BoN10.2359D' 'BoF4.1300H' 'BoN12.1220H' 'BoF3.1205D' 'BoF2.0930D'\n",
            " 'BoN11.1103H' 'BoN10.2359H' 'BoF5.0532H' 'BaF2.2333H' 'SaF2.0534D']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''This block uses the ID's of the train, val and test sets generated above\n",
        "to select the actual recordings. This generates:\n",
        "train_files\n",
        "val_files\n",
        "test_files\n",
        "Which are the arrays of recordings corresponding to each of these '''\n",
        "\n",
        "\n",
        "# Define empty lists\n",
        "val_files = []\n",
        "test_files = []\n",
        "train_files = []\n",
        "\n",
        "# Select all files in dir that have these ID's in their name\n",
        "for f in all_files: #I set this above \n",
        "  #print(f)\n",
        "  namePt1 = f.split(\".\")[0]\n",
        "  namePt2 = f.split(\".\")[1]\n",
        "  ID = namePt1 + '.' + namePt2\n",
        "  #print(ID)\n",
        "  if ID in val_deployments:\n",
        "     val_files.append(f)\n",
        "  if ID in test_deployments:\n",
        "     test_files.append(f)\n",
        "  if ID in train_deployments:\n",
        "     train_files.append(f)\n",
        "\n",
        "print('Number and list of validation files:')\n",
        "print(len(val_files))\n",
        "print(val_files)\n",
        "print('Number and list of test files:')\n",
        "print(len(test_files))\n",
        "print(test_files)\n",
        "print('Number and list of training files:')\n",
        "print(len(train_files))\n",
        "print(train_files)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I9E7FB_mefH",
        "outputId": "ee96b630-fd46-4ec7-c161-5001912fb149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number and list of validation files:\n",
            "14\n",
            "['BaN12.0529H.805322778.180907.1.24.wav', 'BaN12.0529H.805322778.180907.3.48.wav', 'BoF4.2333H.1677983769.180830.4.42.wav', 'BoF4.2333H.1677983769.180830.2.38.wav', 'BoF4.2333H.1677983769.180830.5.54.wav', 'BoN11.1200H.805322778.180906.1.2.wav', 'BoN11.1200H.805322778.180906.3.12.wav', 'BoF5.0532D.671907872.180831.3.14.wav', 'BoN12.1220D.1678278701.180907.4.39.wav', 'BoN12.1220D.1678278701.180907.1.9.wav', 'SaF4.0533D.805322778.180830.2.27.wav', 'SaF4.0533D.805322778.180830.4.41.wav', 'SaF4.0533D.805322778.180830.3.34.wav', 'BaN12.0529H.805322778.180907.4.50.wav']\n",
            "Number and list of test files:\n",
            "15\n",
            "['BoF2.0930H.805322778.180828.3.28.wav', 'BoF2.0930H.805322778.180828.5.53.wav', 'BoF2.0930H.805322778.180828.1.4.wav', 'BoF5.0940H.671907872.180831.2.30.wav', 'BoF5.0940H.671907872.180831.3.36.wav', 'BoF5.0940H.671907872.180831.4.47.wav', 'BoF5.0940D.1677983769.180831.1.1.wav', 'BoF5.0940D.1677983769.180831.2.30.wav', 'SaF5.1202D.1677983769.180831.5.30.wav', 'BoF5.0940D.1677983769.180831.3.34.wav', 'BoF4.1733D.671907872.180830.4.40.wav', 'BoN11.1200D.1678278701.180906.4.15.wav', 'BoN11.1200D.1678278701.180906.5.20.wav', 'SaF5.1202D.1677983769.180831.3.24.wav', 'SaF5.1202D.1677983769.180831.2.17.wav']\n",
            "Number and list of training files:\n",
            "123\n",
            "['BaF1.1055H.1678278701.180827.3.35.wav', 'BaF3.0915H.1678278701.180829.4.52.wav', 'BaF3.0533H.1677983769.180829.3.22.wav', 'BaF2.2333H.1677983769.180828.1.15.wav', 'BaF3.0533H.1677983769.180829.5.59.wav', 'BaF2.1733H.1677983769.180828.3.26.wav', 'BaF3.0533H.1677983769.180829.4.53.wav', 'BaF1.1055H.1678278701.180827.1.1.wav', 'BaF3.0915H.1678278701.180829.1.20.wav', 'BaF1.1055H.1678278701.180827.5.45.wav', 'BaF5.2332H.671907872.180831.3.35.wav', 'BaF5.1330H.671907872.180831.3.18.wav', 'BaF5.1330H.671907872.180831.5.49.wav', 'BaF5.2332H.671907872.180831.4.39.wav', 'BaF5.1732H.671907872.180831.4.31.wav', 'BaF5.1732H.671907872.180831.5.52.wav', 'BaF5.2332H.671907872.180831.1.10.wav', 'BaF4.1040H.1677983769.180830.3.32.wav', 'BaF4.1040H.1677983769.180830.4.35.wav', 'BaF4.1040H.1677983769.180830.1.13.wav', 'BaN11.2330H.805322778.180906.5.50.wav', 'BaN11.2330H.805322778.180906.2.31.wav', 'BaN10.0927H.1677983769.180905.1.10.wav', 'BaN11.2330H.805322778.180906.4.46.wav', 'BaN11.1731H.805322778.180906.2.17.wav', 'BaN11.1731H.805322778.180906.4.27.wav', 'BaN10.0927H.1677983769.180905.2.12.wav', 'BaN11.1315H.805322778.180906.5.55.wav', 'BaN11.1315H.805322778.180906.4.38.wav', 'BaN10.0927H.1677983769.180905.5.39.wav', 'BaN12.0915H.805322778.180907.2.6.wav', 'BoF1.1328H.1678278701.180827.3.24.wav', 'BoF1.1328H.1678278701.180827.1.3.wav', 'BaN12.0915H.805322778.180907.5.59.wav', 'BaN12.0915H.805322778.180907.4.45.wav', 'BoF4.1733H.1677983769.180830.2.21.wav', 'BoF3.1205H.1678278701.180829.4.46.wav', 'BoF4.1300H.1677983769.180830.5.59.wav', 'BoF3.1205H.1678278701.180829.3.30.wav', 'BoF4.1733H.1677983769.180830.1.7.wav', 'BoF3.1205H.1678278701.180829.2.24.wav', 'BoF4.1300H.1677983769.180830.4.57.wav', 'BoF4.1300H.1677983769.180830.2.39.wav', 'BoN10.1731H.1678278701.180905.2.9.wav', 'BoF5.0532H.1677983769.180831.3.37.wav', 'BoF5.0532H.1677983769.180831.5.48.wav', 'BoN10.2359H.1678278701.180905.2.46.wav', 'BoN10.1731H.1678278701.180905.1.5.wav', 'BoF5.0532H.1677983769.180831.4.42.wav', 'BoN11.0529H.1678278701.180906.4.28.wav', 'BoN10.2359H.1678278701.180905.4.54.wav', 'BoN10.2359H.1678278701.180905.5.59.wav', 'BoN11.1103H.805322778.180906.3.21.wav', 'BoN12.1220H.805322778.180907.1.5.wav', 'BoN11.1103H.805322778.180906.1.2.wav', 'BoN11.1103H.805322778.180906.2.15.wav', 'BoN12.1220H.805322778.180907.3.10.wav', 'BoN12.1220H.805322778.180907.4.36.wav', 'BoN10.1731D.805322778.180905.1.7.wav', 'BoF4.2333D.671907872.180830.4.51.wav', 'BoF4.2333D.671907872.180830.3.32.wav', 'BoF4.2333D.671907872.180830.1.3.wav', 'BoN10.2359D.805322778.180905.3.42.wav', 'BoN11.1103D.1678278701.180906.4.32.wav', 'BoN11.1103D.1678278701.180906.3.27.wav', 'BoN10.2359D.805322778.180905.4.55.wav', 'BoN11.1103D.1678278701.180906.5.52.wav', 'BoN11.0529D.805322778.180906.5.50.wav', 'BoN10.1731D.805322778.180905.4.42.wav', 'BoN10.2359D.805322778.180905.5.58.wav', 'BoN11.0529D.805322778.180906.4.32.wav', 'SaF1.1733D.1677983769.180827.1.12.wav', 'SaF1.2333D.1677983769.180827.3.30.wav', 'SaF1.2333D.1677983769.180827.2.15.wav', 'SaF1.2333D.1677983769.180827.5.59.wav', 'SaF1.1733D.1677983769.180827.3.23.wav', 'SaF1.1733D.1677983769.180827.2.16.wav', 'SaF2.0534D.1677983769.180828.1.7.wav', 'SaF2.0534D.1677983769.180828.2.21.wav', 'SaF2.1112D.1678278701.180828.3.16.wav', 'SaF3.1355D.805322778.180829.1.5.wav', 'SaF2.1203D.1678278701.180828.3.33.wav', 'SaF2.0534D.1677983769.180828.3.26.wav', 'SaF2.1112D.1678278701.180828.4.33.wav', 'SaF2.1203D.1678278701.180828.5.43.wav', 'SaF2.1112D.1678278701.180828.5.49.wav', 'SaF2.1203D.1678278701.180828.4.37.wav', 'SaF3.1355D.805322778.180829.2.14.wav', 'SaF3.1733D.805322778.180829.1.17.wav', 'SaF3.2333D.805322778.180829.2.34.wav', 'SaF3.1355D.805322778.180829.3.25.wav', 'SaF3.1733D.805322778.180829.5.59.wav', 'SaF3.2333D.805322778.180829.3.42.wav', 'SaF3.2333D.805322778.180829.4.44.wav', 'SaF3.1733D.805322778.180829.2.26.wav', 'SaN10.1345D.1677983769.180905.5.58.wav', 'SaN11.0940D.1678278701.180906.1.8.wav', 'SaF5.1125D.1677983769.180831.5.33.wav', 'SaN10.1345D.1677983769.180905.1.14.wav', 'SaF5.1125D.1677983769.180831.2.6.wav', 'SaF4.0902D.671907872.180830.3.31.wav', 'SaN11.0940D.1678278701.180906.4.40.wav', 'SaF5.1125D.1677983769.180831.1.1.wav', 'BoF1.1315D.1677983769.180827.4.46.wav', 'BoF2.0930D.1678278701.180828.5.58.wav', 'BoF4.1300D.671907872.180830.1.21.wav', 'BoF2.0930D.1678278701.180828.4.56.wav', 'BoF2.0930D.1678278701.180828.2.27.wav', 'BoF3.1205D.805322778.180829.1.5.wav', 'SaN11.0940D.1678278701.180906.5.59.wav', 'BoF3.1205D.805322778.180829.2.9.wav', 'BoF4.1300D.671907872.180830.2.29.wav', 'BoF1.1315D.1677983769.180827.3.42.wav', 'BoF4.1300D.671907872.180830.3.31.wav', 'BoN10.1731H.1678278701.180905.3.29.wav', 'BaF3.0915H.1678278701.180829.2.30.wav', 'BaF2.1733H.1677983769.180828.2.21.wav', 'BaF2.2333H.1677983769.180828.5.51.wav', 'BaN11.1315H.805322778.180906.3.35.wav', 'BaN11.1731H.805322778.180906.3.24.wav', 'BaF2.1733H.1677983769.180828.4.28.wav', 'BaF2.2333H.1677983769.180828.4.48.wav', 'BoF1.1328H.1678278701.180827.4.28.wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"This block gets mel-specs of each 1s from a file and stores these in arrays \n",
        "for each class that are compatible with the NN execution below. \"\"\"\n",
        "\n",
        "\n",
        "def get_features(file_):\n",
        "    #Loop to calculate log-mel spectrogram features for each file\n",
        "    features_from_files_list = np.empty([0,96,64])\n",
        "        \n",
        "    #calculate features\n",
        "    print('Calculating features for:')\n",
        "    print(file_)\n",
        "    features_from_files_list = np.vstack([features_from_files_list, vggish_input.wavfile_to_examples(file_)])\n",
        "    \n",
        "    #save results in array   \n",
        "    features_from_files = np.array(features_from_files_list)\n",
        "    return features_from_files\n",
        "\n",
        "def prepare_audio(file_, num_of_classes, class_num):                              ################ add more classes as appropriate\n",
        "  \"\"\"\n",
        "  This function cuts larger audio files up into 0.96s chunks and calculates the \n",
        "  logmel spectrograms for each chunk. It then SHUFFLES the output.\n",
        "\n",
        "  Returns:\n",
        "    Two lists, the first contains logmel specs as a numpy array of shape (\n",
        "    [len, num_frames, num_bands] where len is the total number of 0.96s \n",
        "    calculated from all audio in the directory\n",
        "    \n",
        "    the batch_size is variable and\n",
        "    each row is a log mel spectrogram patch of shape [num_frames, num_bands]\n",
        "    suitable for feeding VGGish, while labels is a NumPy array of shape\n",
        "    [batch_size, num_classes] where each row is a multi-hot label vector that\n",
        "    provides the labels for corresponding rows in features.\n",
        "  \"\"\"\n",
        "\n",
        "  # Generate logmel-spec array for each 0.96s of file, this is shape (96,64)*length of file e.g (62, 96, 64) = 1min\n",
        "  logmel_spectrogram = get_features(file_) \n",
        "\n",
        "  # Create one hot coding\n",
        "  class_num = class_num -1 # function takes class going starting at 1, 2, 3 etc, not starting at 0\n",
        "  label = np.zeros(num_of_classes)#, dtype =  np.int8) # Make array of 0's the length of the number of classes\n",
        "  label[class_num] = 1 # One hot code the element in the array correspoding to the class\n",
        "  labels = np.array([label]*len(logmel_spectrogram))#.shape[0]) # Multiple this so it is of the same length as logmel_spectrogram\n",
        "  #labels = labels.astype(int) #wanted to convert y_ to int not float, but can't get it to work\n",
        "\n",
        "\n",
        "  return logmel_spectrogram, labels"
      ],
      "metadata": {
        "id": "gA1Aq5bmBqjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" This block uses the above functions to define more which get the feats and \n",
        "labels for each minibatch and stores these as pickle files to be used by the NN \"\"\"\n",
        "\n",
        "def get_class(filename):\n",
        "    #find part of the name that corresponds to the deployment\n",
        "     #adapted the get_identifier function above to only get class (e.g healthy)\n",
        "    deployment_ID = filename.split(\".\")[1][4:5]\n",
        "    return deployment_ID\n",
        "\n",
        "\n",
        "def pickle_minibatches(minibatch, testvaltrain, minibatch_number):#, test_val_or_train):            #### GOT TO HERE\n",
        "  '''This function extracts logmel-spec usings the functions prepare_audio\n",
        "    defined above. It saves these and the labels in arrays and pickles these up.\n",
        "    This function should be run on a minibatch 1by1\n",
        "    \n",
        "    file_list = a list of .wav files\n",
        "    testvaltrain = 'test', 'val', or 'train' '''\n",
        "\n",
        "  #for i in range(len(minibatch)):\n",
        "  print('Creating minibatch '+str(i))\n",
        "  # Define arrays for each class, add additional classes as appropriate\n",
        "  all_features_class1 = np.empty([0,96,64])\n",
        "  all_labels_class1 = np.empty([0,2])\n",
        "  all_features_class2 = np.empty([0,96,64])\n",
        "  all_labels_class2 = np.empty([0,2])\n",
        "  # for each file in the minibatch, get feats and labels\n",
        "  for k in minibatch:#[i]:\n",
        "    os.chdir(audio_dir)\n",
        "    #print(k)\n",
        "    if get_class(k) == 'H':\n",
        "      features_class1, labels_class1 = prepare_audio(k, _NUM_CLASSES, 1)\n",
        "      all_features_class1 = np.vstack([all_features_class1, features_class1])\n",
        "      all_labels_class1 = np.vstack([all_labels_class1, labels_class1])\n",
        "    if get_class(k) == 'D':\n",
        "      features_class2, labels_class2 = prepare_audio(k, _NUM_CLASSES, 2)\n",
        "      all_features_class2 = np.vstack([all_features_class2, features_class2])\n",
        "      all_labels_class2 = np.vstack([all_labels_class2, labels_class2])\n",
        "  \n",
        "  # Now combine the feats/labels from all classes\n",
        "  minibatch_features = np.vstack([all_features_class1, all_features_class2]) #will need to repeat these lines if using >2 classes\n",
        "  minibatch_labels = np.vstack([all_labels_class1, all_labels_class2])\n",
        "  \n",
        "  # Pickle into the correct folder for train, val or test batches\n",
        "  if testvaltrain == 'train':\n",
        "    #shuffle training data\n",
        "    feats_labels = list(zip(minibatch_features, minibatch_labels)) #zip up\n",
        "    random.shuffle(feats_labels) #shuffle\n",
        "\n",
        "    #save pickle file \n",
        "    os.chdir(pickle_trainfiles_dir)\n",
        "    pickle_filename = testvaltrain + '_' + 'minibatch_' + str(minibatch_number) \n",
        "    with open(pickle_filename, \"wb\") as fp:   #Pickling\n",
        "      pickle.dump(feats_labels, fp)\n",
        "    print('Pickled ' + pickle_filename)\n",
        "\n",
        "  if testvaltrain == 'test':\n",
        "    feats_labels = list(zip(minibatch_features, minibatch_labels))\n",
        "\n",
        "    #save pickle file \n",
        "    os.chdir(pickle_testfiles_dir)\n",
        "    pickle_filename = testvaltrain + '_' + 'minibatch_' + str(minibatch_number) \n",
        "    with open(pickle_filename, \"wb\") as fp:   #Pickling\n",
        "      pickle.dump(feats_labels, fp)\n",
        "    print('Pickled ' + pickle_filename)\n",
        "\n",
        "  if testvaltrain == 'val':\n",
        "    feats_labels = list(zip(minibatch_features, minibatch_labels))\n",
        "\n",
        "    #save pickle file \n",
        "    os.chdir(pickle_valfiles_dir)\n",
        "    pickle_filename = testvaltrain + '_' + 'minibatch_' + str(minibatch_number) \n",
        "    with open(pickle_filename, \"wb\") as fp:   #Pickling\n",
        "      pickle.dump(feats_labels, fp)\n",
        "    print('Pickled ' + pickle_filename)"
      ],
      "metadata": {
        "id": "7n4s__P03L6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute all the above functions on the minibatches and corresponding files"
      ],
      "metadata": {
        "id": "njUhwy5Whu2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Split train, val and test files into minibatches\n",
        " These are give arrays of shape (n, batch_size)\n",
        " e.g 12 minibatches of size 32 would be shape (12, 32)\"\"\"\n",
        " \n",
        "split_minibatches = lambda test_list, x: [test_list[i:i+x] for i in range(0, len(test_list), x)]\n",
        "train_minibatches = split_minibatches(train_files, batch_size)\n",
        "val_minibatches = split_minibatches(val_files, batch_size)\n",
        "test_minibatches = split_minibatches(test_files, batch_size)\n",
        "\n",
        "# Set dir to the directory where all the audio files are stored\n",
        "os.chdir(audio_dir)\n",
        "\n",
        "# Run pickle_minibatches for the train, val and test files, save as pickle files\n",
        "for i in range(len(train_minibatches)):\n",
        "  pickle_minibatches(train_minibatches[i], testvaltrain = 'train', minibatch_number = i)\n",
        "\n",
        "for i in range(len(val_minibatches)):\n",
        "  pickle_minibatches(val_minibatches[i], testvaltrain = 'val', minibatch_number = i)\n",
        "\n",
        "for i in range(len(test_minibatches)):\n",
        "  pickle_minibatches(test_minibatches[i], testvaltrain = 'test', minibatch_number = i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npdCd9mlEiP-",
        "outputId": "9d4c9d0f-f5a6-4b88-f8f4-b751f1e2d8d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating minibatch 0\n",
            "Calculating features for:\n",
            "BaF1.1055H.1678278701.180827.3.35.wav\n",
            "Calculating features for:\n",
            "BaF3.0915H.1678278701.180829.4.52.wav\n",
            "Calculating features for:\n",
            "BaF3.0533H.1677983769.180829.3.22.wav\n",
            "Calculating features for:\n",
            "BaF2.2333H.1677983769.180828.1.15.wav\n",
            "Calculating features for:\n",
            "BaF3.0533H.1677983769.180829.5.59.wav\n",
            "Calculating features for:\n",
            "BaF2.1733H.1677983769.180828.3.26.wav\n",
            "Calculating features for:\n",
            "BaF3.0533H.1677983769.180829.4.53.wav\n",
            "Calculating features for:\n",
            "BaF1.1055H.1678278701.180827.1.1.wav\n",
            "Calculating features for:\n",
            "BaF3.0915H.1678278701.180829.1.20.wav\n",
            "Calculating features for:\n",
            "BaF1.1055H.1678278701.180827.5.45.wav\n",
            "Calculating features for:\n",
            "BaF5.2332H.671907872.180831.3.35.wav\n",
            "Calculating features for:\n",
            "BaF5.1330H.671907872.180831.3.18.wav\n",
            "Calculating features for:\n",
            "BaF5.1330H.671907872.180831.5.49.wav\n",
            "Calculating features for:\n",
            "BaF5.2332H.671907872.180831.4.39.wav\n",
            "Calculating features for:\n",
            "BaF5.1732H.671907872.180831.4.31.wav\n",
            "Calculating features for:\n",
            "BaF5.1732H.671907872.180831.5.52.wav\n",
            "Pickled train_minibatch_0\n",
            "Creating minibatch 1\n",
            "Calculating features for:\n",
            "BaF5.2332H.671907872.180831.1.10.wav\n",
            "Calculating features for:\n",
            "BaF4.1040H.1677983769.180830.3.32.wav\n",
            "Calculating features for:\n",
            "BaF4.1040H.1677983769.180830.4.35.wav\n",
            "Calculating features for:\n",
            "BaF4.1040H.1677983769.180830.1.13.wav\n",
            "Calculating features for:\n",
            "BaN11.2330H.805322778.180906.5.50.wav\n",
            "Calculating features for:\n",
            "BaN11.2330H.805322778.180906.2.31.wav\n",
            "Calculating features for:\n",
            "BaN10.0927H.1677983769.180905.1.10.wav\n",
            "Calculating features for:\n",
            "BaN11.2330H.805322778.180906.4.46.wav\n",
            "Calculating features for:\n",
            "BaN11.1731H.805322778.180906.2.17.wav\n",
            "Calculating features for:\n",
            "BaN11.1731H.805322778.180906.4.27.wav\n",
            "Calculating features for:\n",
            "BaN10.0927H.1677983769.180905.2.12.wav\n",
            "Calculating features for:\n",
            "BaN11.1315H.805322778.180906.5.55.wav\n",
            "Calculating features for:\n",
            "BaN11.1315H.805322778.180906.4.38.wav\n",
            "Calculating features for:\n",
            "BaN10.0927H.1677983769.180905.5.39.wav\n",
            "Calculating features for:\n",
            "BaN12.0915H.805322778.180907.2.6.wav\n",
            "Calculating features for:\n",
            "BoF1.1328H.1678278701.180827.3.24.wav\n",
            "Pickled train_minibatch_1\n",
            "Creating minibatch 2\n",
            "Calculating features for:\n",
            "BoF1.1328H.1678278701.180827.1.3.wav\n",
            "Calculating features for:\n",
            "BaN12.0915H.805322778.180907.5.59.wav\n",
            "Calculating features for:\n",
            "BaN12.0915H.805322778.180907.4.45.wav\n",
            "Calculating features for:\n",
            "BoF4.1733H.1677983769.180830.2.21.wav\n",
            "Calculating features for:\n",
            "BoF3.1205H.1678278701.180829.4.46.wav\n",
            "Calculating features for:\n",
            "BoF4.1300H.1677983769.180830.5.59.wav\n",
            "Calculating features for:\n",
            "BoF3.1205H.1678278701.180829.3.30.wav\n",
            "Calculating features for:\n",
            "BoF4.1733H.1677983769.180830.1.7.wav\n",
            "Calculating features for:\n",
            "BoF3.1205H.1678278701.180829.2.24.wav\n",
            "Calculating features for:\n",
            "BoF4.1300H.1677983769.180830.4.57.wav\n",
            "Calculating features for:\n",
            "BoF4.1300H.1677983769.180830.2.39.wav\n",
            "Calculating features for:\n",
            "BoN10.1731H.1678278701.180905.2.9.wav\n",
            "Calculating features for:\n",
            "BoF5.0532H.1677983769.180831.3.37.wav\n",
            "Calculating features for:\n",
            "BoF5.0532H.1677983769.180831.5.48.wav\n",
            "Calculating features for:\n",
            "BoN10.2359H.1678278701.180905.2.46.wav\n",
            "Calculating features for:\n",
            "BoN10.1731H.1678278701.180905.1.5.wav\n",
            "Pickled train_minibatch_2\n",
            "Creating minibatch 3\n",
            "Calculating features for:\n",
            "BoF5.0532H.1677983769.180831.4.42.wav\n",
            "Calculating features for:\n",
            "BoN11.0529H.1678278701.180906.4.28.wav\n",
            "Calculating features for:\n",
            "BoN10.2359H.1678278701.180905.4.54.wav\n",
            "Calculating features for:\n",
            "BoN10.2359H.1678278701.180905.5.59.wav\n",
            "Calculating features for:\n",
            "BoN11.1103H.805322778.180906.3.21.wav\n",
            "Calculating features for:\n",
            "BoN12.1220H.805322778.180907.1.5.wav\n",
            "Calculating features for:\n",
            "BoN11.1103H.805322778.180906.1.2.wav\n",
            "Calculating features for:\n",
            "BoN11.1103H.805322778.180906.2.15.wav\n",
            "Calculating features for:\n",
            "BoN12.1220H.805322778.180907.3.10.wav\n",
            "Calculating features for:\n",
            "BoN12.1220H.805322778.180907.4.36.wav\n",
            "Calculating features for:\n",
            "BoN10.1731D.805322778.180905.1.7.wav\n",
            "Calculating features for:\n",
            "BoF4.2333D.671907872.180830.4.51.wav\n",
            "Calculating features for:\n",
            "BoF4.2333D.671907872.180830.3.32.wav\n",
            "Calculating features for:\n",
            "BoF4.2333D.671907872.180830.1.3.wav\n",
            "Calculating features for:\n",
            "BoN10.2359D.805322778.180905.3.42.wav\n",
            "Calculating features for:\n",
            "BoN11.1103D.1678278701.180906.4.32.wav\n",
            "Pickled train_minibatch_3\n",
            "Creating minibatch 4\n",
            "Calculating features for:\n",
            "BoN11.1103D.1678278701.180906.3.27.wav\n",
            "Calculating features for:\n",
            "BoN10.2359D.805322778.180905.4.55.wav\n",
            "Calculating features for:\n",
            "BoN11.1103D.1678278701.180906.5.52.wav\n",
            "Calculating features for:\n",
            "BoN11.0529D.805322778.180906.5.50.wav\n",
            "Calculating features for:\n",
            "BoN10.1731D.805322778.180905.4.42.wav\n",
            "Calculating features for:\n",
            "BoN10.2359D.805322778.180905.5.58.wav\n",
            "Calculating features for:\n",
            "BoN11.0529D.805322778.180906.4.32.wav\n",
            "Calculating features for:\n",
            "SaF1.1733D.1677983769.180827.1.12.wav\n",
            "Calculating features for:\n",
            "SaF1.2333D.1677983769.180827.3.30.wav\n",
            "Calculating features for:\n",
            "SaF1.2333D.1677983769.180827.2.15.wav\n",
            "Calculating features for:\n",
            "SaF1.2333D.1677983769.180827.5.59.wav\n",
            "Calculating features for:\n",
            "SaF1.1733D.1677983769.180827.3.23.wav\n",
            "Calculating features for:\n",
            "SaF1.1733D.1677983769.180827.2.16.wav\n",
            "Calculating features for:\n",
            "SaF2.0534D.1677983769.180828.1.7.wav\n",
            "Calculating features for:\n",
            "SaF2.0534D.1677983769.180828.2.21.wav\n",
            "Calculating features for:\n",
            "SaF2.1112D.1678278701.180828.3.16.wav\n",
            "Pickled train_minibatch_4\n",
            "Creating minibatch 5\n",
            "Calculating features for:\n",
            "SaF3.1355D.805322778.180829.1.5.wav\n",
            "Calculating features for:\n",
            "SaF2.1203D.1678278701.180828.3.33.wav\n",
            "Calculating features for:\n",
            "SaF2.0534D.1677983769.180828.3.26.wav\n",
            "Calculating features for:\n",
            "SaF2.1112D.1678278701.180828.4.33.wav\n",
            "Calculating features for:\n",
            "SaF2.1203D.1678278701.180828.5.43.wav\n",
            "Calculating features for:\n",
            "SaF2.1112D.1678278701.180828.5.49.wav\n",
            "Calculating features for:\n",
            "SaF2.1203D.1678278701.180828.4.37.wav\n",
            "Calculating features for:\n",
            "SaF3.1355D.805322778.180829.2.14.wav\n",
            "Calculating features for:\n",
            "SaF3.1733D.805322778.180829.1.17.wav\n",
            "Calculating features for:\n",
            "SaF3.2333D.805322778.180829.2.34.wav\n",
            "Calculating features for:\n",
            "SaF3.1355D.805322778.180829.3.25.wav\n",
            "Calculating features for:\n",
            "SaF3.1733D.805322778.180829.5.59.wav\n",
            "Calculating features for:\n",
            "SaF3.2333D.805322778.180829.3.42.wav\n",
            "Calculating features for:\n",
            "SaF3.2333D.805322778.180829.4.44.wav\n",
            "Calculating features for:\n",
            "SaF3.1733D.805322778.180829.2.26.wav\n",
            "Calculating features for:\n",
            "SaN10.1345D.1677983769.180905.5.58.wav\n",
            "Pickled train_minibatch_5\n",
            "Creating minibatch 6\n",
            "Calculating features for:\n",
            "SaN11.0940D.1678278701.180906.1.8.wav\n",
            "Calculating features for:\n",
            "SaF5.1125D.1677983769.180831.5.33.wav\n",
            "Calculating features for:\n",
            "SaN10.1345D.1677983769.180905.1.14.wav\n",
            "Calculating features for:\n",
            "SaF5.1125D.1677983769.180831.2.6.wav\n",
            "Calculating features for:\n",
            "SaF4.0902D.671907872.180830.3.31.wav\n",
            "Calculating features for:\n",
            "SaN11.0940D.1678278701.180906.4.40.wav\n",
            "Calculating features for:\n",
            "SaF5.1125D.1677983769.180831.1.1.wav\n",
            "Calculating features for:\n",
            "BoF1.1315D.1677983769.180827.4.46.wav\n",
            "Calculating features for:\n",
            "BoF2.0930D.1678278701.180828.5.58.wav\n",
            "Calculating features for:\n",
            "BoF4.1300D.671907872.180830.1.21.wav\n",
            "Calculating features for:\n",
            "BoF2.0930D.1678278701.180828.4.56.wav\n",
            "Calculating features for:\n",
            "BoF2.0930D.1678278701.180828.2.27.wav\n",
            "Calculating features for:\n",
            "BoF3.1205D.805322778.180829.1.5.wav\n",
            "Calculating features for:\n",
            "SaN11.0940D.1678278701.180906.5.59.wav\n",
            "Calculating features for:\n",
            "BoF3.1205D.805322778.180829.2.9.wav\n",
            "Calculating features for:\n",
            "BoF4.1300D.671907872.180830.2.29.wav\n",
            "Pickled train_minibatch_6\n",
            "Creating minibatch 7\n",
            "Calculating features for:\n",
            "BoF1.1315D.1677983769.180827.3.42.wav\n",
            "Calculating features for:\n",
            "BoF4.1300D.671907872.180830.3.31.wav\n",
            "Calculating features for:\n",
            "BoN10.1731H.1678278701.180905.3.29.wav\n",
            "Calculating features for:\n",
            "BaF3.0915H.1678278701.180829.2.30.wav\n",
            "Calculating features for:\n",
            "BaF2.1733H.1677983769.180828.2.21.wav\n",
            "Calculating features for:\n",
            "BaF2.2333H.1677983769.180828.5.51.wav\n",
            "Calculating features for:\n",
            "BaN11.1315H.805322778.180906.3.35.wav\n",
            "Calculating features for:\n",
            "BaN11.1731H.805322778.180906.3.24.wav\n",
            "Calculating features for:\n",
            "BaF2.1733H.1677983769.180828.4.28.wav\n",
            "Calculating features for:\n",
            "BaF2.2333H.1677983769.180828.4.48.wav\n",
            "Calculating features for:\n",
            "BoF1.1328H.1678278701.180827.4.28.wav\n",
            "Pickled train_minibatch_7\n",
            "Creating minibatch 0\n",
            "Calculating features for:\n",
            "BaN12.0529H.805322778.180907.1.24.wav\n",
            "Calculating features for:\n",
            "BaN12.0529H.805322778.180907.3.48.wav\n",
            "Calculating features for:\n",
            "BoF4.2333H.1677983769.180830.4.42.wav\n",
            "Calculating features for:\n",
            "BoF4.2333H.1677983769.180830.2.38.wav\n",
            "Calculating features for:\n",
            "BoF4.2333H.1677983769.180830.5.54.wav\n",
            "Calculating features for:\n",
            "BoN11.1200H.805322778.180906.1.2.wav\n",
            "Calculating features for:\n",
            "BoN11.1200H.805322778.180906.3.12.wav\n",
            "Calculating features for:\n",
            "BoF5.0532D.671907872.180831.3.14.wav\n",
            "Calculating features for:\n",
            "BoN12.1220D.1678278701.180907.4.39.wav\n",
            "Calculating features for:\n",
            "BoN12.1220D.1678278701.180907.1.9.wav\n",
            "Calculating features for:\n",
            "SaF4.0533D.805322778.180830.2.27.wav\n",
            "Calculating features for:\n",
            "SaF4.0533D.805322778.180830.4.41.wav\n",
            "Calculating features for:\n",
            "SaF4.0533D.805322778.180830.3.34.wav\n",
            "Calculating features for:\n",
            "BaN12.0529H.805322778.180907.4.50.wav\n",
            "Pickled val_minibatch_0\n",
            "Creating minibatch 0\n",
            "Calculating features for:\n",
            "BoF2.0930H.805322778.180828.3.28.wav\n",
            "Calculating features for:\n",
            "BoF2.0930H.805322778.180828.5.53.wav\n",
            "Calculating features for:\n",
            "BoF2.0930H.805322778.180828.1.4.wav\n",
            "Calculating features for:\n",
            "BoF5.0940H.671907872.180831.2.30.wav\n",
            "Calculating features for:\n",
            "BoF5.0940H.671907872.180831.3.36.wav\n",
            "Calculating features for:\n",
            "BoF5.0940H.671907872.180831.4.47.wav\n",
            "Calculating features for:\n",
            "BoF5.0940D.1677983769.180831.1.1.wav\n",
            "Calculating features for:\n",
            "BoF5.0940D.1677983769.180831.2.30.wav\n",
            "Calculating features for:\n",
            "SaF5.1202D.1677983769.180831.5.30.wav\n",
            "Calculating features for:\n",
            "BoF5.0940D.1677983769.180831.3.34.wav\n",
            "Calculating features for:\n",
            "BoF4.1733D.671907872.180830.4.40.wav\n",
            "Calculating features for:\n",
            "BoN11.1200D.1678278701.180906.4.15.wav\n",
            "Calculating features for:\n",
            "BoN11.1200D.1678278701.180906.5.20.wav\n",
            "Calculating features for:\n",
            "SaF5.1202D.1677983769.180831.3.24.wav\n",
            "Calculating features for:\n",
            "SaF5.1202D.1677983769.180831.2.17.wav\n",
            "Pickled test_minibatch_0\n"
          ]
        }
      ]
    }
  ]
}