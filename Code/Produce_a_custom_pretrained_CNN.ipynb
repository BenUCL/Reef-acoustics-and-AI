{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenUCL/Reef-acoustics-and-AI/blob/main/Code/Produce_a_custom_pretrained_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLRcmHXBrd50"
      },
      "source": [
        "# **Train the CNN on your audio, save this as a pretrained model**\n",
        "\n",
        "This script provides an example of training the CNN on the minibatch files which can be created with the 'CNN minibatch creation' script. This uses a small subset of the Indonesian dataset.\n",
        "\n",
        "This outputs a saved version of the model, such that it can be used as a new custom pretrained version of th CNN to extract features from your own audio. This is in the form of four files which are saved to the '/Results/trained_CNN_saved_model/' folder. These are called:\n",
        "\n",
        "\n",
        "1.   custom_pretrained_CNN.ckpt.meta\n",
        "2.   custom_pretrained_CNN.ckpt.index\n",
        "3. custom_pretrained_CNN.ckpt.data-00000-of-00001\n",
        "4. checkpoint\n",
        "\n",
        "\n",
        "To use this custom pretrained CNN, you will need to:\n",
        "\n",
        "1.   Duplicate the Audioset folder and name this 'Custom Audioset'  \n",
        "2. Delete the vggish_model.ckpt file and copy in the four new files.\n",
        "3. Open the 'AudiosetAnalysis.py' file in this folder and replace the line: *self.checkpoint_path = 'vggish_model.ckpt'* with: *self.checkpoint_path = 'custom_pretrained_CNN.ckpt'*\n",
        "4. Save this.\n",
        "5. You can then open the 'Feature extraction with pretrained CNN.pynb' and change the path given to the *vggish_files =* line to the path for your new Audioset folder.\n",
        "6. This will now extract features on your audio files exactly as before, but this time it uses the custom pretrained CNN you have created\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **Using Colabs free GPU feature**\n",
        "\n",
        "Google colab provides free GPU access (with some limits), see here: https://research.google.com/colaboratory/faq.html\n",
        "\n",
        "This can be used to significantly increase training speed. To switch this on go to 'Runtime' at the top and change type to 'GPU'.\n",
        "\n",
        "\n",
        "### **If you use this code, please cite Williams et al (2023)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFSJ2V4TmVSq",
        "outputId": "9700c76d-1f85-48ce-ea90-0c6d1baedfbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connect your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h2jq6cOCT7Ca",
        "outputId": "bb1ddb2a-675d-4798-9990-33528a94be96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.21.5\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting resampy==0.2.2\n",
            "  Downloading resampy-0.2.2.tar.gz (323 kB)\n",
            "\u001b[K     |████████████████████████████████| 323 kB 60.0 MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.15\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 27 kB/s \n",
            "\u001b[?25hCollecting tf_slim==1.1.0\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 69.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
            "Collecting soundfile==0.10.3.post1\n",
            "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: scipy>=0.13 in /usr/local/lib/python3.7/dist-packages (from resampy==0.2.2) (1.7.3)\n",
            "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.7/dist-packages (from resampy==0.2.2) (0.56.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (2.0.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 55.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.14.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 69.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.49.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.3.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile==0.10.3.post1) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile==0.10.3.post1) (2.21)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.32->resampy==0.2.2) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.32->resampy==0.2.2) (4.13.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.32->resampy==0.2.2) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.32->resampy==0.2.2) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.32->resampy==0.2.2) (3.9.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
            "Building wheels for collected packages: resampy, gast\n",
            "  Building wheel for resampy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320732 sha256=9334778748613d39590f7b77913d601c7fa134a6fea6b92b521e37196e2b9852\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/18/0a/8ad18a597d8333a142c9789338a96a6208f1198d290ece356c\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=86969895b9a24a06eec392fb870e6b65e656f143013789c4c1d8f29f3f1ea6e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built resampy gast\n",
            "Installing collected packages: numpy, tensorflow-estimator, tensorboard, keras-applications, gast, tf-slim, tensorflow, soundfile, resampy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "  Attempting uninstall: soundfile\n",
            "    Found existing installation: soundfile 0.11.0\n",
            "    Uninstalling soundfile-0.11.0:\n",
            "      Successfully uninstalled soundfile-0.11.0\n",
            "  Attempting uninstall: resampy\n",
            "    Found existing installation: resampy 0.4.2\n",
            "    Uninstalling resampy-0.4.2:\n",
            "      Successfully uninstalled resampy-0.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 numpy-1.21.5 resampy-0.2.2 soundfile-0.10.3.post1 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 tf-slim-1.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" As package versions began updating this threw errors on the smoke test. \\nFor a faster download versions could be removed but this may throw errors. \\nAs of 17/10/22 it gives the below output, but, the smoketest codeblock passes:\\n\\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\\ntensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\\nkapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\\nSuccessfully installed gast-0.2.2 keras-applications-1.0.8 llvmlite-0.32.1 numba-0.49.1 numpy-1.21.5 resampy-0.2.2 soundfile-0.10.3.post1 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 tf-slim-1.1.0\\nWARNING: The following packages were previously imported in this runtime:\\n  [numpy]\\nYou must restart the runtime in order to use newly installed versions. \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip install numpy==1.21.5 resampy==0.2.2 tensorflow==1.15 tf_slim==1.1.0 six==1.15.0 soundfile==0.10.3.post1\n",
        "\n",
        "\"\"\" As package versions began updating this threw errors on the smoke test. \n",
        "For a faster download versions could be removed but this may throw errors. \n",
        "As of 17/10/22 it gives the below output, but, the smoketest codeblock passes:\n",
        "\n",
        "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
        "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
        "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\n",
        "Successfully installed gast-0.2.2 keras-applications-1.0.8 llvmlite-0.32.1 numba-0.49.1 numpy-1.21.5 resampy-0.2.2 soundfile-0.10.3.post1 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 tf-slim-1.1.0\n",
        "WARNING: The following packages were previously imported in this runtime:\n",
        "  [numpy]\n",
        "You must restart the runtime in order to use newly installed versions. \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV91rjp5T7KW",
        "outputId": "da0a895e-65bf-4b40-fcb0-c5b9445d913a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Reef soundscapes with AI/Audioset\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "\n",
            "Testing your install of VGGish\n",
            "\n",
            "Log Mel Spectrogram example:  [[-4.47297436 -4.29457354 -4.14940631 ... -3.9747003  -3.94774997\n",
            "  -3.78687669]\n",
            " [-4.48589533 -4.28825497 -4.139964   ... -3.98368686 -3.94976505\n",
            "  -3.7951698 ]\n",
            " [-4.46158065 -4.29329706 -4.14905953 ... -3.96442484 -3.94895483\n",
            "  -3.78619839]\n",
            " ...\n",
            " [-4.46152626 -4.29365061 -4.14848608 ... -3.96638113 -3.95057575\n",
            "  -3.78538167]\n",
            " [-4.46152595 -4.2936572  -4.14848104 ... -3.96640507 -3.95059567\n",
            "  -3.78537143]\n",
            " [-4.46152565 -4.29366386 -4.14847603 ... -3.96642906 -3.95061564\n",
            "  -3.78536116]]\n",
            "2022-10-19 17:40:39.605886: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2022-10-19 17:40:39.723109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-10-19 17:40:39.723934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2022-10-19 17:40:39.732031: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-19 17:40:39.732310: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-19 17:40:39.733247: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-19 17:40:39.733445: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-19 17:40:39.733616: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-19 17:40:39.733770: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-19 17:40:39.734678: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-19 17:40:39.734709: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2022-10-19 17:40:39.735180: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-10-19 17:40:39.749263: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
            "2022-10-19 17:40:39.749443: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3d424c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-10-19 17:40:39.749474: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2022-10-19 17:40:40.078330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-10-19 17:40:40.079342: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3d42680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2022-10-19 17:40:40.079386: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2022-10-19 17:40:40.079544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2022-10-19 17:40:40.079570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1666: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "VGGish embedding:  [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.16137294 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.80695784\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.36792755 0.03582418 0.         0.         0.\n",
            " 0.         0.38027036 0.13755938 0.9174708  0.80656356 0.\n",
            " 0.         0.         0.         0.04036269 0.7076244  0.\n",
            " 0.4978391  0.24081807 0.21565425 0.884923   1.1956801  0.67061985\n",
            " 0.2077946  0.01639876 0.17471859 0.         0.         0.25100812\n",
            " 0.         0.         0.14607906 0.         0.39887053 0.30542108\n",
            " 0.1289675  0.         0.         0.         0.         0.\n",
            " 0.5385135  0.         0.         0.04941082 0.42527413 0.18537286\n",
            " 0.         0.         0.1475353  0.         0.         0.6993387\n",
            " 0.45541185 0.05174828 0.         0.01992539 0.         0.\n",
            " 0.5181578  0.56557596 0.6587975  0.         0.         0.41056335\n",
            " 0.         0.         0.         0.25765198 0.23232105 0.24026453\n",
            " 0.         0.         0.         0.         0.         0.2652375\n",
            " 0.         0.48460817 0.         0.         0.19325784 0.\n",
            " 0.20123352 0.         0.03368617 0.         0.         0.\n",
            " 0.         0.17836353 0.024749   0.06889973 0.         0.\n",
            " 0.         0.08246295 0.         0.         0.         0.\n",
            " 0.         0.        ]\n",
            "\n",
            "Looks Good To Me!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Should output 'Looks good to me at the bottom!'\n",
        "%cd /content/drive/MyDrive/Reef soundscapes with AI/Audioset\n",
        "!python vggish_smoke_test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZdEHlqrT7M6",
        "outputId": "faf65f8a-9ae0-401a-d1f3-1c5df8d82e3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "#From original vggish_train_demo.py script on github\n",
        "from __future__ import print_function\n",
        "\n",
        "from random import shuffle\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tf_slim as slim\n",
        "\n",
        "import vggish_input\n",
        "import vggish_params\n",
        "import vggish_slim\n",
        "\n",
        "#Modules added by Ben\n",
        "import os #for handling directories\n",
        "import glob #for dealing with files in dir\n",
        "import pandas as pd #for saving output at end in dataframe\n",
        "import sklearn\n",
        "import math\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split #added for train/test split\n",
        "from numpy import loadtxt #addded so predictions can be output to CSV file\n",
        "from datetime import datetime #added to append time to csv output file name to prevent overwriting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTz1EZxtiRa9"
      },
      "source": [
        "**Set paths to access modules and pickle files, also set CNN parameters.**\n",
        "\n",
        "Two classes are used here, increase _NUM_CLASSES if needed. A batch size of 16 was used as larger batches can cause a memory error on colab depending on which GPU you are  allocated. The network trains for 5 epochs currently to save computation time, the final study used UCL's computing cluster to train for 50 epochs on the full datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0zN9KAOcWziG"
      },
      "outputs": [],
      "source": [
        "#which repeat of the cross-val is this? (1-8):\n",
        "repeat = 1 # Used to set seed for train/val/test split\n",
        "\n",
        "### Change paths if you re-structure folders\n",
        "\n",
        "output_name = 'custom_pretrained_CNN.ckpt' # what to call the ckpt file output\n",
        "\n",
        "\n",
        "# Path to the location where your audio file are stored:\n",
        "audio_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/audio_dir' \n",
        "\n",
        "# Path to folder containing vggish setup files and 'AudiosetAnalysis' downloaded from sarebs supplementary\n",
        "vggish_files = r'/content/drive/MyDrive/Reef soundscapes with AI/Audioset' \n",
        "\n",
        "# Output folder for results:\n",
        "results_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/Results/trained_CNN_saved_model/' \n",
        "ckpt_file_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/Results/trained_CNN_saved_model/' \n",
        "\n",
        "#Set the directories where logmel-spectrograms will be stored for train, test and validation sets:\n",
        "pickle_trainfiles_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/Results/minibatches_train/'\n",
        "pickle_valfiles_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/Results/minibatches_val/'\n",
        "pickle_testfiles_dir = r'/content/drive/MyDrive/Reef soundscapes with AI/Results/minibatches_test/'\n",
        "\n",
        "#how many classes?:\n",
        "_NUM_CLASSES = 2\n",
        "\n",
        "#name a column for each class e.g 'class1', 'class2', or 'healthy', 'degraded'\n",
        "col_names = 'Healthy','Degraded', 'True class'\n",
        "\n",
        "#Batch size:\n",
        "batch_size = 16 # larger batches can cause a memory error on the NN script on colab depending on which GPU you are  allocated \n",
        "\n",
        "# Number of epochs.\n",
        "num_epochs = 10 # set less to 1 to run a quick demo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apTYZYvDnbZN",
        "outputId": "82e25065-23aa-40ce-9faa-251c50fd3a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train minibatches found: 8\n",
            "Number of validation minibatches found: 1\n",
            "Number of test minibatches found: 1\n",
            "Cross validation combination: 1\n"
          ]
        }
      ],
      "source": [
        "#### Some final set up\n",
        "# Find number of minibatches for networks for loop\n",
        "minibatches = [filename for filename in os.listdir(pickle_trainfiles_dir) if filename.startswith(\"train_minibatch\")]\n",
        "num_minibatches = len(minibatches) #this takes the last digit of the last pickle files, denoting how many minibatches there are\n",
        "\n",
        "# Get number of train/test/val minibatches\n",
        "num_train_batches = int(len(os.listdir(pickle_trainfiles_dir)))\n",
        "print('Number of train minibatches found: ' + str(num_train_batches))\n",
        "num_val_batches = int(len(os.listdir(pickle_valfiles_dir)))\n",
        "print('Number of validation minibatches found: ' + str(num_val_batches))\n",
        "num_test_batches = int(len(os.listdir(pickle_testfiles_dir)))\n",
        "print('Number of test minibatches found: ' + str(num_test_batches))\n",
        "\n",
        "os.chdir(vggish_files) \n",
        "\n",
        "# Used to find averages of accuracy score across minibatches later\n",
        "def Average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "\n",
        "print('Cross validation combination: ' + str(repeat))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thPSDwZmk2Dv"
      },
      "source": [
        "# **Run the neural network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fZuMo8UDkI55"
      },
      "outputs": [],
      "source": [
        "#RUN THIS BLOCK ONLY ONCE PER SESSION - otherwise it will error\n",
        "\n",
        "flags = tf.app.flags\n",
        "\n",
        "flags.DEFINE_boolean(\n",
        "    'train_vggish', True,\n",
        "    'If True, allow VGGish parameters to change during training, thus '\n",
        "    'fine-tuning VGGish. If False, VGGish parameters are fixed, thus using '\n",
        "    'VGGish as a fixed feature extractor.')\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    'checkpoint', 'vggish_model.ckpt',\n",
        "    'Path to the VGGish checkpoint file.')\n",
        "\n",
        "FLAGS = flags.FLAGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dckDf8YewAX8"
      },
      "source": [
        "'An exception has occurred, use %tb to see the full traceback.' error will occur, fear not, this just means its finished "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "7E0YTlu4xNvt",
        "outputId": "ecf3aa36-9e2d-4c96-8170-f5eaa5798b40"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I1019 17:55:10.762954 140493836892032 saver.py:1284] Restoring parameters from vggish_model.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mini batch0\n",
            "Epoch 1 completed out of 10 , loss: 0.8381498456001282\n",
            "mini batch0\n",
            "Epoch 2 completed out of 10 , loss: 0.6784347295761108\n",
            "mini batch0\n",
            "Epoch 3 completed out of 10 , loss: 0.5777193307876587\n",
            "mini batch0\n",
            "Epoch 4 completed out of 10 , loss: 0.5069913268089294\n",
            "mini batch0\n",
            "Epoch 5 completed out of 10 , loss: 0.4453549087047577\n",
            "mini batch0\n",
            "Epoch 6 completed out of 10 , loss: 0.38863372802734375\n",
            "mini batch0\n",
            "Epoch 7 completed out of 10 , loss: 0.33752354979515076\n",
            "mini batch0\n",
            "Epoch 8 completed out of 10 , loss: 0.29111260175704956\n",
            "mini batch0\n",
            "Epoch 9 completed out of 10 , loss: 0.2465088814496994\n",
            "mini batch0\n",
            "Epoch 10 completed out of 10 , loss: 0.20115140080451965\n",
            "Lowest loss: []\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "\"\"\"To train 5 epochs on the 123x1min files in the of training data this process\n",
        "takes up to 80 minutes on a CPU. Depending which GPU Colab provides you this can \n",
        "take < 5min on colabs GPU. The final study used NVIDIA A100 GPU's which \n",
        "generally provide the highest speed as of 2022.\"\"\"\n",
        "\n",
        "\n",
        "### Train NN, output results\n",
        "r\"\"\"This uses the VGGish model definition within a larger model which adds two \n",
        "layers on top, and then trains this larger model. \n",
        "\n",
        "We input log-mel spectrograms (X_train) calculated above with associated labels \n",
        "(y_train), and feed the batches into the model. Once the model is trained, it \n",
        "is then executed on the validation and log-mel spectrograms (X_validation, \n",
        "X_test), and the accuracy is output for each.\n",
        "\n",
        "This version of the CNN then saves .ckpt files which are saved versions of\n",
        "this model which can be used as a pretrained feature extractor, created using \n",
        "your own audio\"\"\"\n",
        "\n",
        "def main(X):   \n",
        "  with tf.Graph().as_default(), tf.Session() as sess:\n",
        "    # Define VGGish.\n",
        "    embeddings = vggish_slim.define_vggish_slim(training=FLAGS.train_vggish)\n",
        "    \n",
        "    \n",
        "    # Define a shallow classification model and associated training ops on top\n",
        "    # of VGGish.\n",
        "    with tf.variable_scope('mymodel'):\n",
        "      # Add a fully connected layer with 100 units. Add an activation function\n",
        "      # to the embeddings since they are pre-activation.\n",
        "      num_units = 100\n",
        "      fc = slim.fully_connected(tf.nn.relu(embeddings), num_units)\n",
        "\n",
        "      logits= slim.fully_connected(                                      \n",
        "          fc, _NUM_CLASSES, activation_fn=None, scope='logits')\n",
        "      probabilities = tf.sigmoid(logits, name='probabilities')\n",
        "    \n",
        "      # Add training ops.\n",
        "      with tf.variable_scope('train'):\n",
        "        global_step = tf.train.create_global_step()\n",
        "\n",
        "        # Labels are assumed to be fed as a batch multi-hot vectors, with\n",
        "        # a 1 in the position of each positive class label, and 0 elsewhere.\n",
        "        labels_input = tf.placeholder(\n",
        "            tf.float32, shape=(None, _NUM_CLASSES), name='labels')\n",
        "\n",
        "        # Cross-entropy label loss.\n",
        "        xent = tf.nn.softmax_cross_entropy_with_logits( \n",
        "            logits=logits, labels=labels_input, name='xent')     \n",
        "        loss = tf.reduce_mean(xent, name='loss_op')\n",
        "        tf.summary.scalar('loss', loss)\n",
        "\n",
        "        # We use the same optimizer and hyperparameters as used to train VGGish.\n",
        "        optimizer = tf.train.AdamOptimizer(\n",
        "            learning_rate= vggish_params.LEARNING_RATE,     \n",
        "            epsilon=vggish_params.ADAM_EPSILON)\n",
        "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
        "\n",
        "    # Initialize all variables in the model, and then load the pre-trained\n",
        "    # VGGish checkpoint.\n",
        "    sess.run(tf.global_variables_initializer())         ### this starts the session appaz\n",
        "    vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n",
        "\n",
        "    \n",
        "    features_input = sess.graph.get_tensor_by_name(\n",
        "        vggish_params.INPUT_TENSOR_NAME)\n",
        "    \n",
        "    # The training loop.\n",
        "    saver = tf.train.Saver()####\n",
        "    all_loss = []\n",
        "    for epoch in range(num_epochs):\n",
        "            validation_accuracy_scores = []\n",
        "            test_accuracy_scores = []\n",
        "            test_batch_scores = []\n",
        "            val_batch_scores = []\n",
        "            epoch_loss = 0\n",
        "            i=0\n",
        "            while i < num_minibatches: \n",
        "                print('mini batch'+str(i))\n",
        "                train_pickle_file = pickle_trainfiles_dir + 'train_minibatch_' + str(i)\n",
        "                with open(train_pickle_file, \"rb\") as fp:   # Unpickling\n",
        "                  batch = pickle.load(fp)\n",
        "                batch_x, batch_y = zip(*batch)\n",
        "\n",
        "                _, c = sess.run([train_op, loss], feed_dict={features_input: batch_x, labels_input: batch_y})\n",
        "                epoch_loss += c\n",
        "                i+=1\n",
        "            #print no. of epochs and loss\n",
        "            print('Epoch', epoch+1, 'completed out of', num_epochs,', loss:',epoch_loss) \n",
        "\n",
        "    all_loss.sort()\n",
        "    print('Lowest loss: ' + str(all_loss[:1]))\n",
        "    os.chdir(ckpt_file_dir)\n",
        "    saver.save(sess, output_name)  ## double space into for loop to get each epoch\n",
        "    \n",
        "tf.app.run(main)   "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "igHVDQpBYn1H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyNxRQilHR12xM9Dtm6xqczA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}